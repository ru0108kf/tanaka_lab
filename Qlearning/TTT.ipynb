{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "609d2ba0",
   "metadata": {},
   "source": [
    "# 三目並べゲーム\n",
    "参考:https://github.com/narisan25/TTT-RL\n",
    "## ボードと環境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a745b70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.10.14)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "\n",
    "# POS State\n",
    "EMPTY = 0  # 空の状態を表す定数\n",
    "PLAYER_X = 1  # プレイヤーXを表す定数\n",
    "PLAYER_O = -1  # プレイヤーOを表す定数\n",
    "DRAW = 2  # 引き分けを表す定数\n",
    "\n",
    "class TTTBoard:\n",
    "    \"\"\"ボードを管理するクラス\"\"\"\n",
    "    def __init__(self, board=None):\n",
    "        if board is None:\n",
    "            self.board = [EMPTY] * 9  # ボードを初期化\n",
    "        else:\n",
    "            self.board = board\n",
    "        self.winner = None  # 勝者を保持する変数\n",
    "        self.invalid_act = None  # 無効な行動を保持する変数\n",
    "    \n",
    "    def get_possible_pos(self):\n",
    "        \"\"\"可能な位置を取得\"\"\"\n",
    "        return [i for i, cell in enumerate(self.board) if cell == EMPTY]  # 空の位置をリストで返す\n",
    "    \n",
    "    def pygame_init(self):\n",
    "        \"\"\"pygame開始\"\"\"\n",
    "        pygame.init()  # pygameを初期化\n",
    "        self.screen = pygame.display.set_mode((300, 300))  # ウィンドウのサイズを設定\n",
    "        self.font = pygame.font.Font(None, 100)  # フォントを設定\n",
    "        pygame.display.set_caption(\"Tic Tac Toe\")  # ウィンドウのタイトルを設定\n",
    "        self.pygame_render(self.board)  # 初期描画\n",
    "    \n",
    "    def pygame_render(self, board):\n",
    "        \"\"\"pygame描画\"\"\"\n",
    "        WHITE = (255, 255, 255)  # 白色\n",
    "        BLACK = (0, 0, 0)  # 黒色\n",
    "        \n",
    "        self.screen.fill(WHITE)  # 画面を白で塗りつぶす\n",
    "        \n",
    "        for x in range(1, 3):\n",
    "            pygame.draw.line(self.screen, BLACK, (x * 100, 0), (x * 100, 300), 3)  # 垂直線を描画\n",
    "            pygame.draw.line(self.screen, BLACK, (0, x * 100), (300, x * 100), 3)  # 水平線を描画\n",
    "            \n",
    "        for i in range(9):\n",
    "            x = i % 3\n",
    "            y = i // 3\n",
    "            if board[i] == PLAYER_X:\n",
    "                text = self.font.render('X', True, BLACK)  # 'X'を描画\n",
    "                self.screen.blit(text, (x * 100 + 25, y * 100 + 15))\n",
    "            elif board[i] == PLAYER_O:\n",
    "                text = self.font.render('O', True, BLACK)  # 'O'を描画\n",
    "                self.screen.blit(text, (x * 100 + 25, y * 100 + 15))\n",
    "        \n",
    "        pygame.display.flip()  # 画面を更新\n",
    "\n",
    "    def check_winner(self):\n",
    "        \"\"\"勝ちを確認\"\"\"\n",
    "        win_cond = ((0, 1, 2), (3, 4, 5), (6, 7, 8), (0, 3, 6), (1, 4, 7), (2, 5, 8), (0, 4, 8), (2, 4, 6))  # 勝利条件\n",
    "        for each in win_cond:\n",
    "            if self.board[each[0]] == self.board[each[1]] == self.board[each[2]] and self.board[each[0]] != EMPTY:\n",
    "                self.winner = self.board[each[0]]  # 勝者を設定\n",
    "                return self.winner\n",
    "        return None\n",
    "    \n",
    "    def check_draw(self):\n",
    "        \"\"\"引き分けを確認\"\"\"\n",
    "        if not any(cell == EMPTY for cell in self.board) and self.winner is None:\n",
    "            self.winner = DRAW  # 引き分けを設定\n",
    "            return DRAW\n",
    "        return None\n",
    "    \n",
    "    def step(self, pos, player):\n",
    "        \"\"\"次の状態\"\"\"\n",
    "        if self.board[pos] == EMPTY:\n",
    "            self.board[pos] = player  # プレイヤーの位置を設定\n",
    "            self.invalid_act = None\n",
    "        else:\n",
    "            self.invalid_act = pos  # 無効な位置を設定\n",
    "            self.winner = -1 * player  # 相手の勝利を設定\n",
    "        self.check_winner()  # 勝利の確認\n",
    "        self.check_draw()  # 引き分けの確認\n",
    "    \n",
    "    def clone(self):\n",
    "        return TTTBoard(self.board.copy())  # ボードのコピーを返す\n",
    "\n",
    "    def switch_player(self):\n",
    "        if self.player_turn == PLAYER_X:\n",
    "            self.player_turn = PLAYER_O  # プレイヤーを切り替える\n",
    "        else:\n",
    "            self.player_turn = PLAYER_X  # プレイヤーを切り替える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de72e4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TTTenv:\n",
    "\n",
    "    act_turn = 0  # アクションターンの初期化\n",
    "    winner = None  # 勝者の初期化\n",
    "    \n",
    "    def __init__(self, px, po, nplay=-1, showBoard=True, showResult=True, stat=100):\n",
    "        self.player_x = px  # プレイヤーXを設定\n",
    "        self.player_o = po  # プレイヤーOを設定\n",
    "        self.nwon = {px.myturn: 0, po.myturn: 0, DRAW: 0}  # 勝利数の初期化\n",
    "        self.nplay = nplay  # プレイ回数の設定\n",
    "        self.players = (self.player_x, self.player_o)  # プレイヤーのタプル\n",
    "        self.board = None  # ボードの初期化\n",
    "        self.disp = showBoard  # ボード表示の設定\n",
    "        self.showResult = showResult  # 結果表示の設定\n",
    "        self.player_turn = self.players[random.randrange(2)]  # ランダムにプレイヤーのターンを決定\n",
    "        self.nplayed = 0  # プレイ済みの回数\n",
    "        self.stat = stat  # 統計の表示間隔\n",
    "    \n",
    "    def progress(self, random_turn=True):\n",
    "        \"\"\"ゲームの進行\"\"\"\n",
    "        while self.nplayed != self.nplay:\n",
    "            if random_turn:\n",
    "                self.player_turn = self.players[random.randrange(2)]  # ランダムにプレイヤーのターンを決定\n",
    "            self.board = TTTBoard()  # 新しいボードを作成\n",
    "            if self.disp:\n",
    "                self.board.pygame_init()  # pygameの初期化と表示\n",
    "            while self.board.winner is None:\n",
    "                act = self.player_turn.act(self.board)  # プレイヤーの行動を取得\n",
    "                self.board.step(act, self.player_turn.myturn)  # 行動をボードに反映\n",
    "                if self.disp: self.board.pygame_render(self.board.board)  # ボードを描画\n",
    "               \n",
    "                if self.board.winner is not None:\n",
    "                    for i in self.players:\n",
    "                        i.getGameResult(self.board)  # 各プレイヤーに結果を通知\n",
    "                    if self.board.winner == DRAW:\n",
    "                        if self.showResult: print(\"Draw Game\")  # 引き分けの場合の表示\n",
    "                    elif self.board.winner == self.player_turn.myturn:\n",
    "                        out = \"Winner : \" + self.player_turn.name  # 勝者の表示\n",
    "                        if self.showResult: print(out)\n",
    "                    else:\n",
    "                        print(\"Invalid Act!\")  # 無効な行動の表示\n",
    "                    self.nwon[self.board.winner] += 1  # 勝利数を更新\n",
    "                else:\n",
    "                    self.switch_player()  # プレイヤーのターンを切り替える\n",
    "                    self.player_turn.getGameResult(self.board)  # 現在のボードの結果を取得\n",
    "\n",
    "            self.nplayed += 1  # プレイ済みの回数を更新\n",
    "            if self.nplayed % self.stat == 0 or self.nplayed == self.nplay:\n",
    "                print(f\"{self.player_x.name}:{self.nwon[self.player_x.myturn]},\\\n",
    "                {self.player_o.name}:{self.nwon[self.player_o.myturn]},\\\n",
    "                DRAW:{self.nwon[DRAW]}\")  # 統計の表示\n",
    "                if self.disp: pygame.quit()  # pygameを終了\n",
    "                    \n",
    "    def switch_player(self):\n",
    "        \"\"\"プレイヤーのターンを切り替える\"\"\"\n",
    "        if self.player_turn == self.player_x:\n",
    "            self.player_turn = self.player_o  # プレイヤーOに切り替え\n",
    "        else:\n",
    "            self.player_turn = self.player_x  # プレイヤーXに切り替え"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2926050",
   "metadata": {},
   "source": [
    "## ランダムとランダムαと人間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07c1637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "class RandomAgent:\n",
    "    \"\"\"ただのランダム\"\"\"\n",
    "    def __init__(self, turn):\n",
    "        self.name = \"Random\"\n",
    "        self.myturn = turn\n",
    "        \n",
    "    def act(self, board):\n",
    "        acts = board.get_possible_pos()\n",
    "        i = random.randrange(len(acts))\n",
    "        return acts[i]\n",
    "    \n",
    "    def getGameResult(self, board):\n",
    "        pass\n",
    "\n",
    "class AlphaRandomAgent:\n",
    "    \"\"\"勝てるところがあれば勝ちに行くランダム\"\"\"\n",
    "    \n",
    "    def __init__(self,turn,name=\"AlphaRandom\"):\n",
    "        self.name=name\n",
    "        self.myturn=turn\n",
    "        \n",
    "    def getGameResult(self,winner):\n",
    "        pass\n",
    "        \n",
    "    def act(self,board):\n",
    "        acts=board.get_possible_pos()\n",
    "        #see only next winnable act\n",
    "        for act in acts:\n",
    "            tempboard=board.clone()\n",
    "            tempboard.step(act,self.myturn)\n",
    "            # check if win\n",
    "            if tempboard.winner==self.myturn:\n",
    "                #print (\"Check mate\")\n",
    "                return act\n",
    "        i=random.randrange(len(acts))\n",
    "        return acts[i]\n",
    "\n",
    "\n",
    "class HumanAgent:\n",
    "    \"\"\"人が操作する\"\"\"\n",
    "    def __init__(self, turn):\n",
    "        self.name = \"Human\"\n",
    "        self.myturn = turn\n",
    "        \n",
    "    def act(self, board):\n",
    "        valid = False\n",
    "        while not valid:\n",
    "            for event in pygame.event.get():# Pygameのイベントを処理する\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    sys.exit()\n",
    "                elif event.type == pygame.MOUSEBUTTONDOWN:# マウスの位置を取得する\n",
    "                    \n",
    "                    x, y = event.pos # マウスの位置をボードのセルに変換する\n",
    "                    row = y // 100\n",
    "                    col = x // 100\n",
    "                    act = row * 3 + col\n",
    "                    if act >= 0 and act < 9 and board.board[act] == EMPTY:\n",
    "                        valid = True\n",
    "                        return act\n",
    "                    else:\n",
    "                        board.pygame_render(board.board)\n",
    "                        RED = (255, 0, 0, 150)  # 赤色（透明度150）    \n",
    "                        pygame.draw.rect(board.screen, RED, (col * 100, row * 100, 100, 100), 3)  # 無効な位置を赤で囲む\n",
    "                        pygame.display.flip()  # 画面を更新することで描画が反映される\n",
    "    \n",
    "    def getGameResult(self, board):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46685339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vsAlphaRandom(): \n",
    "    p1=HumanAgent(PLAYER_X)\n",
    "    p2=AlphaRandomAgent(PLAYER_O)\n",
    "    game=TTTenv(p1,p2)\n",
    "    game.progress()\n",
    "#vsAlphaRandom()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfd5fb5",
   "metadata": {},
   "source": [
    "## モンテカルロ法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c80ca1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAgent:\n",
    "    def __init__(self,turn,name=\"MC\"):\n",
    "        self.name=name\n",
    "        self.myturn=turn\n",
    "    \n",
    "    def getGameResult(self,winner):\n",
    "        pass\n",
    "        \n",
    "    def win_or_rand(self,board,turn):\n",
    "        acts=board.get_possible_pos()\n",
    "        #see only next winnable act\n",
    "        for act in acts:\n",
    "            tempboard=board.clone()\n",
    "            tempboard.step(act,turn)\n",
    "            # check if win\n",
    "            if tempboard.winner==turn:\n",
    "                return act\n",
    "        i=random.randrange(len(acts))\n",
    "        return acts[i]\n",
    "           \n",
    "    def trial(self,score,board,act):\n",
    "        tempboard=board.clone()\n",
    "        tempboard.step(act,self.myturn)\n",
    "        tempturn=self.myturn\n",
    "        while tempboard.winner is None:\n",
    "            tempturn=tempturn*-1\n",
    "            tempboard.step(self.win_or_rand(tempboard,tempturn),tempturn)\n",
    "        \n",
    "        if tempboard.winner==self.myturn:\n",
    "            score[act]+=1\n",
    "        elif tempboard.winner==DRAW:\n",
    "            pass\n",
    "        else:\n",
    "            score[act]-=1\n",
    "\n",
    "        \n",
    "    def getGameResult(self,board):\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    def act(self,board):\n",
    "        acts=board.get_possible_pos()\n",
    "        scores={}\n",
    "        n=50\n",
    "        for act in acts:\n",
    "            scores[act]=0\n",
    "            for i in range(n):\n",
    "                #print(\"Try\"+str(i))\n",
    "                self.trial(scores,board,act)\n",
    "            \n",
    "            #print(scores)\n",
    "            scores[act]/=n\n",
    "        \n",
    "        max_score=max(scores.values())\n",
    "        for act, v in scores.items():\n",
    "            if v == max_score:\n",
    "                #print(str(act)+\"=\"+str(v))\n",
    "                return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ab48302",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def vsMC():\n",
    "    p1=HumanAgent(PLAYER_X)\n",
    "    p2=MCAgent(PLAYER_O,\"M2\")\n",
    "    game=TTTenv(p1,p2)\n",
    "    game.progress()\n",
    "#vsMC()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0878ce0",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c6490b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class QLAgent:\n",
    "    def __init__(self, turn, name=\"QL\", epsilon=0.2, alpha=0.3):\n",
    "        self.name = name  # エージェントの名前\n",
    "        self.myturn = turn  # エージェントのターン（PLAYER_X or PLAYER_O）\n",
    "        self.q = {}  # Qテーブル (state, action) -> Q値 のマッピング\n",
    "        self.epsilon = epsilon  # ε-greedy の ε パラメータ (探索の確率)\n",
    "        self.alpha = alpha  # 学習率\n",
    "        self.gamma = 0.9  # 割引率\n",
    "        self.last_move = None  # 前回の行動\n",
    "        self.last_board = None  # 前回のボードの状態\n",
    "        self.totalgamecount = 0  # 総ゲーム数\n",
    "        \n",
    "    def act(self, board):\n",
    "        \"\"\"エージェントの行動を決定する\"\"\"\n",
    "        self.last_board = board.clone()  # 現在のボード状態を保存\n",
    "        acts = board.get_possible_pos()  # 可能な行動を取得\n",
    "\n",
    "        # ε-greedy 探索：ランダムに行動を選択\n",
    "        if random.random() < (self.epsilon / (self.totalgamecount // 10000 + 1)):\n",
    "            i = random.randrange(len(acts))\n",
    "            return acts[i]\n",
    "\n",
    "        # Q値に基づいて行動を選択\n",
    "        qs = [self.getQ(tuple(self.last_board.board), act) for act in acts]\n",
    "        maxQ = max(qs)\n",
    "\n",
    "        if qs.count(maxQ) > 1:\n",
    "            # 最良の選択肢が複数ある場合、それらからランダムに選択\n",
    "            best_options = [i for i in range(len(acts)) if qs[i] == maxQ]\n",
    "            i = random.choice(best_options)\n",
    "        else:\n",
    "            i = qs.index(maxQ)\n",
    "\n",
    "        self.last_move = acts[i]\n",
    "        return acts[i]\n",
    "    \n",
    "    def getQ(self, state, act):\n",
    "        \"\"\"Q値を取得する\"\"\"\n",
    "        if self.q.get((state, act)) is None:\n",
    "            self.q[(state, act)] = 1  # 初期Q値を1とする\n",
    "        return self.q.get((state, act))\n",
    "    \n",
    "    def getGameResult(self, board):\n",
    "        \"\"\"ゲーム結果を処理する\"\"\"\n",
    "        r = 0\n",
    "        if self.last_move is not None:\n",
    "            if board.winner is None:\n",
    "                self.learn(self.last_board, self.last_move, 0, board)\n",
    "            else:\n",
    "                if board.winner == self.myturn:\n",
    "                    self.learn(self.last_board, self.last_move, 1, board)\n",
    "                elif board.winner != DRAW:\n",
    "                    self.learn(self.last_board, self.last_move, -1, board)\n",
    "                else:\n",
    "                    self.learn(self.last_board, self.last_move, 0, board)\n",
    "                self.totalgamecount += 1\n",
    "                self.last_move = None\n",
    "                self.last_board = None\n",
    "\n",
    "    def learn(self, s, a, r, fs):\n",
    "        \"\"\"Q学習の更新を行う\"\"\"\n",
    "        pQ = self.getQ(tuple(s.board), a)\n",
    "        if fs.winner is not None:\n",
    "            maxQnew = 0\n",
    "        else:\n",
    "            maxQnew = max([self.getQ(tuple(fs.board), act) for act in fs.get_possible_pos()])\n",
    "        self.q[(tuple(s.board), a)] = pQ + self.alpha * ((r + self.gamma * maxQnew) - pQ)\n",
    "\n",
    "    def save_weights(self, filepath='agt_data/noname'):\n",
    "        \"\"\"Qテーブルをファイルに保存する\"\"\"\n",
    "        filepath = filepath + '.pkl'\n",
    "        with open(filepath, mode='wb') as f:\n",
    "            pickle.dump(self.q, f)\n",
    "\n",
    "    def load_weights(self, filepath='agt_data/noname'):\n",
    "        \"\"\"ファイルからQテーブルを読み込む\"\"\"\n",
    "        filepath = filepath + '.pkl'\n",
    "        with open(filepath, mode='rb') as f:\n",
    "            self.q = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59088b4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def trainQL():\n",
    "    p1=QLAgent(PLAYER_O,\"QL1\")\n",
    "    p2=QLAgent(PLAYER_X,\"QL2\")\n",
    "    game=TTTenv(p1,p2,100000,False,False,10000)\n",
    "    game.progress()\n",
    "    p1.save_weights(filepath='agt_data/tictactoe_QL')\n",
    "#trainQL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddc75691",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msy-t\\anaconda3\\envs\\DL\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def vsQL():\n",
    "    p1=QLAgent(PLAYER_O,\"QL1\")\n",
    "    p1.epsilon=0\n",
    "    p2=HumanAgent(PLAYER_X)\n",
    "    p1.load_weights(filepath='agt_data/tictactoe_QL')\n",
    "    game=TTTenv(p1,p2)\n",
    "    game.player_turn = game.players[1]# 先攻\n",
    "    game.progress(random_turn=False)\n",
    "vsQL()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadf7fa2",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d97d01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, turn, name=\"DQN\",\n",
    "                 gamma = 0.95, # 割引率\n",
    "                 epsilon = 1, # 乱雑度\n",
    "                ):\n",
    "        \n",
    "        # パラメータ\n",
    "        self.name = name  # エージェントの名前\n",
    "        self.myturn = turn  # エージェントのターン（PLAYER_X or PLAYER_O）\n",
    "        self.input_size = (9,)\n",
    "        self.n_act = 9\n",
    "        self.gamma = gamma \n",
    "        self.epsilon = epsilon  \n",
    "        self.model = self._build_Qnet()\n",
    "        self.totalgamecount = 0\n",
    "        self.last_move = None\n",
    "        self.last_board = None\n",
    "        self.last_pred = None\n",
    "        # 報酬パラメータ\n",
    "        self.rwin, self.rlose, self.rdraw, self.rmiss = 1, -1, 0, -1.5\n",
    "                \n",
    "    def _build_Qnet(self):\n",
    "        \"\"\"Qネットワークの構築\"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=self.input_size))\n",
    "        model.add(Dense(162, activation='relu'))\n",
    "        model.add(Dense(162, activation='relu'))\n",
    "        model.add(Dense(self.n_act, activation='linear'))\n",
    "        \n",
    "        # 勾配法のパラメータの定義\n",
    "        model.compile(loss='mse', optimizer='Adam')\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def act(self, board):\n",
    "        \"\"\"行動を決定\"\"\"\n",
    "        self.last_board = board.clone()  # 現在のボード状態を保存。clone() メソッドを使ってオブジェクトをコピー。\n",
    "        x = np.array([board.board], dtype=np.float32)\n",
    "        \n",
    "        pred = self.model.predict(x, verbose=0)[0, :]  # モデルに入力データを渡し、予測値を取得。\n",
    "        self.last_pred = pred\n",
    "        act = np.argmax(pred)  # 予測値の中で最大値を持つインデックスを取得し、行動を決定。\n",
    "        \n",
    "        if self.epsilon > 0.2:  # εを時間経過とともに減少させる（ε-greedy アルゴリズム）。\n",
    "            self.epsilon -= 1 / 20000\n",
    "        if random.random() < self.epsilon:  # εの確率でランダムな行動を選択。\n",
    "            acts = board.get_possible_pos()  # 可能な行動を取得。\n",
    "            i = random.randrange(len(acts))  # ランダムに行動を選択。\n",
    "            act = acts[i]\n",
    "        i = 0\n",
    "        while board.board[act] != EMPTY:  # 無効な行動の場合、繰り返し有効な行動を見つけるまでループ。\n",
    "            self.learn(self.last_board, act, -1, self.last_board)  # 無効な行動の場合、報酬-1として学習。\n",
    "            x = np.array([board.board], dtype=np.float32)\n",
    "            pred = self.model.predict(x, verbose=0)[0, :]  # モデルに入力データを渡し、再度予測値を取得。\n",
    "            act = np.argmax(pred)\n",
    "            i += 1\n",
    "            if i > 10:  # 10回以上繰り返しても有効な行動が見つからない場合の処理。\n",
    "                print(\"Exceed Pos Find\" + str(board.board) + \" with \" + str(act))\n",
    "                acts = self.last_board.get_possible_pos()  # 前回のボード状態から可能な行動を取得。\n",
    "                act = acts[random.randrange(len(acts))]\n",
    "\n",
    "        self.last_move = act  # 最終的に選択された行動を保存。\n",
    "        return act  # 選択された行動を返す。\n",
    "            \n",
    "    def getGameResult(self, board):\n",
    "        \"\"\"ゲームの結果を処理\"\"\"\n",
    "        r = 0\n",
    "        if self.last_move is not None:  # 前の行動が存在する場合にのみ処理を行う\n",
    "            if board.winner is None:  # 勝者が決まっていない場合\n",
    "                self.learn(self.last_board, self.last_move, 0, board)  # 報酬0で学習\n",
    "            else:\n",
    "                if board.board == self.last_board.board:  # ボード状態が前回と同じ場合\n",
    "                    self.learn(self.last_board, self.last_move, self.rmiss, board)  # 誤った行動として学習\n",
    "                elif board.winner == self.myturn:  # 自分が勝者の場合\n",
    "                    self.learn(self.last_board, self.last_move, self.rwin, board)  # 勝利の報酬で学習\n",
    "                elif board.winner != DRAW:  # 相手が勝者の場合\n",
    "                    self.learn(self.last_board, self.last_move, self.rlose, board)  # 敗北の報酬で学習\n",
    "                else:  # 引き分けの場合\n",
    "                    self.learn(self.last_board, self.last_move, self.rdraw, board)  # 引き分けの報酬で学習\n",
    "                self.totalgamecount += 1  # 総ゲーム数をインクリメント\n",
    "                self.last_move = None  # 前回の行動をリセット\n",
    "                self.last_board = None  # 前回のボード状態をリセット\n",
    "                self.last_pred = None  # 前回の予測結果をリセット\n",
    "             \n",
    "    def learn(self, obs, act, rwd, next_obs):\n",
    "        \"\"\"学習\"\"\"\n",
    "        # ゲームの次の状態(next_obs)の勝者が存在するかをチェック\n",
    "        if next_obs.winner is not None:\n",
    "            maxQnew = 0  # 勝者が存在する場合、次の状態の最大Q値は0とする\n",
    "        else:\n",
    "            # モデルに次の状態を入力し、予測値の最大値を取得\n",
    "            x = np.array([next_obs.board], dtype=np.float32)\n",
    "            Q = self.model.predict(x, verbose=0)[0, :]\n",
    "            maxQnew = np.max(Q)\n",
    "        \n",
    "        # Q値の更新式に基づいて、更新するQ値を計算\n",
    "        update = rwd + self.gamma * maxQnew\n",
    "\n",
    "        # 前回の行動に対するQ値を更新\n",
    "        self.last_pred[act] = update\n",
    "        \n",
    "        x = np.array([obs.board], dtype=np.float32)\n",
    "        t = np.array([self.last_pred], dtype=np.float32)\n",
    "        \n",
    "        self.model.fit(x, t, verbose=0, epochs=1)     \n",
    "    \n",
    "    def save_weights(self, filepath='agt_data/noname'):\n",
    "        \"\"\"モデルを保存する\"\"\"\n",
    "        self.model.save(filepath + '.keras', overwrite=True)\n",
    "\n",
    "    def load_weights(self, filepath='agt_data/noname'):\n",
    "        \"\"\"モデルの重みを読み込む\"\"\"\n",
    "        self.model = tf.keras.models.load_model(filepath + '.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e83319",
   "metadata": {},
   "outputs": [],
   "source": [
    "pDQ=DQNAgent(PLAYER_X)\n",
    "p2=AlphaRandomAgent(PLAYER_O)\n",
    "game=TTTenv(pDQ,p2,1000,False,False,100)\n",
    "game.progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e25824",
   "metadata": {},
   "outputs": [],
   "source": [
    "pDQ.e=1\n",
    "pQ=QLAgent(PLAYER_O,\"QL1\")\n",
    "pQ.epsilon=0\n",
    "pQ.load_weights(filepath='agt_data/tictactoe_QL')\n",
    "game=TTTenv(pDQ,pQ,30000,False,False,1000)\n",
    "game.progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4e1c4e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# リストを定義\n",
    "data = [[0,0,0],[0,0,0],[0,0,0]]\n",
    "\n",
    "# NumPy配列に変換\n",
    "data_array = np.array(data)\n",
    "print(data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6d0c506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "# 現在の形状を確認\n",
    "print(data_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5060b74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# NumPy配列をテンソルに変換\n",
    "data_tensor = tf.convert_to_tensor(data_array, dtype=tf.float32)\n",
    "print(data_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f64fed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]], shape=(1, 3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# バッチ次元を追加\n",
    "data_tensor = tf.expand_dims(data_tensor, axis=0)\n",
    "print(data_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca7676b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]], shape=(1, 10), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msy-t\\anaconda3\\envs\\DL\\lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 簡単なモデルの定義\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(3, 3)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# モデルのコンパイル\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# モデルへの入力\n",
    "output = model(data_tensor)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6d1eac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
