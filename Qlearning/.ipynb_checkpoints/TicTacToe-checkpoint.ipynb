{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c52bb88",
   "metadata": {},
   "source": [
    "# 三目並べゲーム\n",
    "## 環境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de72e4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.10.14)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "import sys\n",
    "\n",
    "class TicTacToeEnv:\n",
    "    def __init__(self, size = 3, player_turn=random.choice([1, -1])):\n",
    "        self.size = size \n",
    "        self.board = [[0 for _ in range(size)] for _ in range(size)]  # サイズをもとにボードを作成\n",
    "        self.player_turn = player_turn # プレイヤーのターン\n",
    "        self.winner = None  # 勝者\n",
    "        self.invalid_move = None  # 無効な動作\n",
    "        \n",
    "    def game_reset(self):\n",
    "        \"\"\"ボードの初期化\"\"\"\n",
    "        self.board = [[0 for _ in range(self.size)] for _ in range(self.size)] \n",
    "        self.player_turn = random.choice([1, -1])\n",
    "        self.winner = None\n",
    "        self.invalid_move = None\n",
    "    \n",
    "    def done(self):\n",
    "        \"\"\"終了判定\"\"\"\n",
    "        # 各行と列をチェック\n",
    "        for i in range(self.size):\n",
    "            # 行をチェック\n",
    "            if self.board[i].count(self.player_turn) == self.size:\n",
    "                self.winner = self.player_turn\n",
    "                return True\n",
    "            # 列をチェック\n",
    "            column = [self.board[j][i] for j in range(self.size)]\n",
    "            if column.count(self.player_turn) == self.size:\n",
    "                self.winner = self.player_turn\n",
    "                return True\n",
    "\n",
    "        # 主対角線をチェック\n",
    "        if all(self.board[i][i] == self.player_turn for i in range(self.size)):\n",
    "            self.winner = self.player_turn\n",
    "            return True\n",
    "\n",
    "        # 副対角線をチェック\n",
    "        if all(self.board[i][self.size - 1 - i] == self.player_turn for i in range(self.size)):\n",
    "            self.winner = self.player_turn\n",
    "            return True\n",
    "        \n",
    "        # ドローのチェック\n",
    "        if all(self.board[i][j] != 0 for i in range(self.size) for j in range(self.size)):\n",
    "            self.winner = 0  # ドローの時は勝者を0とする\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def step(self, act):\n",
    "        \"\"\"状態を更新\"\"\"\n",
    "        x, y = divmod(act, self.size)\n",
    "        # actを受け取って、次の状態にする\n",
    "        if self.board[x][y] == 0:  # 受け取ったactが有効なら\n",
    "            self.board[x][y] = self.player_turn\n",
    "            self.invalid_move = None\n",
    "            done = self.done()\n",
    "            # プレイヤー交代\n",
    "            self.player_turn *= -1\n",
    "            return done\n",
    "        else:\n",
    "            self.invalid_move = (x, y)\n",
    "            return None\n",
    "    \n",
    "    def pygame_init(self):\n",
    "        \"\"\"pygame開始\"\"\"\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((self.size * 100, self.size * 100))\n",
    "        self.font = pygame.font.Font(None, 100)\n",
    "        pygame.display.set_caption(\"Tic Tac Toe　Game\")\n",
    "        self.pygame_render()\n",
    "\n",
    "    def pygame_render(self):\n",
    "        \"\"\"描画関数\"\"\"\n",
    "        # pygameでboardの内容を描画する\n",
    "        WHITE = (255, 255, 255)\n",
    "        BLACK = (0, 0, 0)\n",
    "        RED = (255, 0, 0, 150)\n",
    "        self.screen.fill(WHITE)\n",
    "        for x in range(1, self.size):\n",
    "            pygame.draw.line(self.screen, BLACK, (x * 100, 0), (x * 100, self.size * 100), 3)\n",
    "            pygame.draw.line(self.screen, BLACK, (0, x * 100), (self.size * 100, x * 100), 3)\n",
    "            \n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                if self.board[i][j] == 1:\n",
    "                    text = self.font.render('O', True, BLACK)\n",
    "                    self.screen.blit(text, (j * 100 + 25, i * 100 + 15))\n",
    "                elif self.board[i][j] == -1:\n",
    "                    text = self.font.render('X', True, BLACK)\n",
    "                    self.screen.blit(text, (j * 100 + 25, i * 100 + 15))\n",
    "            \n",
    "        if self.invalid_move is not None: \n",
    "            (i, j) = self.invalid_move\n",
    "            pygame.draw.rect(self.screen, RED, (j * 100, i * 100, 100, 100), 3)\n",
    "        \n",
    "        pygame.display.flip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47a70e11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def progress(agent1, agent2, size=3, render=True, nplay=-1):\n",
    "    \"\"\"ゲーム進行\"\"\"\n",
    "    env = TicTacToeEnv(size)\n",
    "    \n",
    "    # 勝敗のカウント\n",
    "    wins_O, wins_X, draws = 0,0,0\n",
    "    \n",
    "    if render:\n",
    "        env.pygame_init()  # Pygameを初期化し、ゲームウィンドウを設定する\n",
    "    running = True\n",
    "    \n",
    "    while running and sum([wins_O,wins_X,draws])!=nplay:\n",
    "        if render:\n",
    "            env.pygame_render()\n",
    "        # 行動を決める\n",
    "        if env.player_turn == 1:\n",
    "            act = agent1.act(env)\n",
    "        elif env.player_turn == -1:\n",
    "            act = agent2.act(env)\n",
    "        \n",
    "        done = env.step(act)\n",
    "        \n",
    "        if render:\n",
    "            env.pygame_render()\n",
    "        \n",
    "        # 終了判定 \n",
    "        if done:\n",
    "            if env.winner == 1:\n",
    "                wins_O += 1\n",
    "            elif env.winner == -1:\n",
    "                wins_X += 1\n",
    "            else:\n",
    "                draws += 1\n",
    "            print(f\"Winner: {'O   ' if env.winner == 1 else 'X   ' if env.winner == -1 else 'Draw'}\", end='\\r')\n",
    "            # 初期化\n",
    "            env.game_reset()\n",
    "            \n",
    "        if render:\n",
    "            for event in pygame.event.get():  # Pygameのイベントを処理する\n",
    "                if event.type == pygame.QUIT:  # ウィンドウの閉じるボタンがクリックされたとき\n",
    "                    running = False  # メインループを終了する\n",
    "    \n",
    "    print(f\"Final Results: O wins: {wins_O}, X wins: {wins_X}, Draws: {draws}\")\n",
    "\n",
    "    pygame.quit()  # Pygameを終了する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2926050",
   "metadata": {},
   "source": [
    "## ランダムとランダムαと人間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07c1637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \"\"\"完全ランダム\"\"\"\n",
    "    def __init__(self, my_turn):\n",
    "        self.my_turn = my_turn\n",
    "\n",
    "    def act(self, env):\n",
    "        act = random.choice(acts)\n",
    "        return act\n",
    "\n",
    "class RandomalfaAgent:\n",
    "    \"\"\"勝てるところがあれば勝つランダム\"\"\"\n",
    "    def __init__(self, my_turn):\n",
    "        self.my_turn = my_turn\n",
    "        \n",
    "    def act(self, env):\n",
    "        possible_acts =  [i * env.size + j for i in range(env.size) for j in range(env.size) if env.board[i][j] == 0]\n",
    "        for act in possible_acts:\n",
    "            x, y = divmod(act, env.size)\n",
    "            if env.board[x][y] == 0:\n",
    "                env.board[x][y] = self.my_turn\n",
    "                env.done()\n",
    "                if env.winner == self.my_turn:\n",
    "                    act = x * env.size + y\n",
    "                    env.board[x][y] = 0\n",
    "                    env.winner = None\n",
    "                    return act\n",
    "                env.board[x][y] = 0\n",
    "                    \n",
    "        act = random.choice(possible_acts)\n",
    "        return act\n",
    "\n",
    "class HumanAgent:\n",
    "    \"\"\"人がプレイヤーのクラス\"\"\"\n",
    "    def __init__(self, my_turn):\n",
    "        self.my_turn = my_turn\n",
    "\n",
    "    def act(self, env):\n",
    "        while True:\n",
    "            for event in pygame.event.get():  # Pygameのイベントを処理する\n",
    "                if event.type == pygame.MOUSEBUTTONDOWN and event.button == 1:\n",
    "                    pos = pygame.mouse.get_pos()  # マウスの位置を取得する\n",
    "                    x, y = pos[1] // 100, pos[0] // 100  # マウスの位置をボードのセルに変換する\n",
    "                    act = x * env.size + y  # 行と列を1次元のインデックスに変換する\n",
    "                    if env.board[x][y] == 0:\n",
    "                        return act\n",
    "                elif event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1be96e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Results: O wins: 50, X wins: 48, Draws: 2\n"
     ]
    }
   ],
   "source": [
    "agent1 = RandomalfaAgent(1)\n",
    "agent2 = RandomalfaAgent(-1)\n",
    "progress(agent1,agent2,render=True,nplay=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfd5fb5",
   "metadata": {},
   "source": [
    "# モンテカルロ法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "843835ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class MCAgent:\n",
    "    \"\"\"モンテカルロ法\"\"\"\n",
    "    def __init__(self, my_turn):\n",
    "        self.my_turn = my_turn\n",
    "\n",
    "    def win_or_rand(self, env, turn):\n",
    "        \"\"\"次の手で勝てる場合、勝ち、そうでないならランダムに動く\"\"\"\n",
    "        possible_acts =  [i * env.size + j for i in range(env.size) for j in range(env.size) if env.board[i][j] == 0]\n",
    "        for act in possible_acts:\n",
    "            x, y = divmod(act, env.size)\n",
    "            if env.board[x][y] == 0:\n",
    "                env.board[x][y] = turn\n",
    "                env.done()\n",
    "                if env.winner == turn:\n",
    "                    act = x * env.size + y\n",
    "                    env.board[x][y] = 0\n",
    "                    env.winner = None\n",
    "                    return act\n",
    "                env.board[x][y] = 0\n",
    "        \n",
    "        act = random.choice(possible_acts)\n",
    "        return act\n",
    "\n",
    "    def trial(self, scores, env, act):\n",
    "        tempboard = deepcopy(env.board)\n",
    "        tempturn = self.my_turn\n",
    "        done = env.step(act)\n",
    "        \n",
    "        while not done:\n",
    "            tempturn *= -1\n",
    "            tempact = self.win_or_rand(env,tempturn)\n",
    "            done = env.step(tempact)\n",
    "        if env.winner == self.my_turn:\n",
    "            scores[act] += 1\n",
    "        elif env.winner == self.my_turn*-1:\n",
    "            scores[act] += -1\n",
    "        env.player_turn = self.my_turn\n",
    "        env.board = tempboard\n",
    "        env.winner = None\n",
    "        \n",
    "    def act(self, env):\n",
    "        scores={}\n",
    "        n=50\n",
    "        possible_acts =  [i * env.size + j for i in range(env.size) for j in range(env.size) if env.board[i][j] == 0]\n",
    "        for act in possible_acts:\n",
    "            scores[act]=0\n",
    "            for i in range(n):\n",
    "                self.trial(scores,env,act)\n",
    "            scores[act]/=n\n",
    "        max_score=max(scores.values())\n",
    "        for act, v in scores.items():\n",
    "            if v == max_score:\n",
    "                return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30af6524",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msy-t\\anaconda3\\envs\\DL\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "agent1 = MCAgent(1)\n",
    "agent2 = MCAgent(-1)\n",
    "progress(agent1,agent2,size=3,nplay=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca0ab96",
   "metadata": {},
   "source": [
    "# QLarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "822ae670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class QLAgent:\n",
    "    def __init__(self, my_turn, \n",
    "                 gamma=0.9,    # 割引率\n",
    "                 epsilon=0.2,  # 乱雑度\n",
    "                 alpha=0.3,    # 学習率\n",
    "                 memory_size=10000):\n",
    "        \n",
    "        # パラメータ\n",
    "        self.my_turn = my_turn\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.init_val_Q = 1\n",
    "        self.memory_size = memory_size\n",
    "        self.last_obss = deque(maxlen=2)\n",
    "        self.last_act = None\n",
    "        \n",
    "        # Qテーブル関連\n",
    "        self.Q = {}     # Qテーブル\n",
    "        self.len_Q = 0  # Qテーブルに登録した観測の数\n",
    "        self.episode = 0\n",
    "        \n",
    "        self.filepath = 'agt_data/tictactoe_QL'\n",
    "\n",
    "    def act(self, env):\n",
    "        \"\"\"観測に対して行動を出力\"\"\"\n",
    "        obs = env.board\n",
    "        possible_acts = [i * env.size + j for i in range(env.size) for j in range(env.size) if env.board[i][j] == 0]\n",
    "        \n",
    "        # obsを文字列に変換\n",
    "        obs = str(obs)\n",
    "        \n",
    "        # obs が登録されていなかったら初期値を与えて登録\n",
    "        self._check_and_add_observation(obs, possible_acts)\n",
    "        \n",
    "        # 確率的に処理を分岐\n",
    "        if random.random() < (self.epsilon/(self.episode//10000+1)):\n",
    "            # epsilon の確率\n",
    "            act = random.choice(possible_acts)  # ランダム行動\n",
    "        else:\n",
    "            # 1-epsilon の確率\n",
    "            acts_q_values = self.Q[obs]\n",
    "            max_q_value = max(acts_q_values.values()) # 最大のQ値\n",
    "            # 最大のQ値が複数ある場合の処理\n",
    "            max_q_acts = [act for act, q_value in acts_q_values.items() if q_value == max_q_value] \n",
    "            act = random.choice(max_q_acts)\n",
    "        return act\n",
    "\n",
    "    def _check_and_add_observation(self, obs, possible_acts):\n",
    "        \"\"\"obs が登録されていなかったら初期値を与えて登録\"\"\"\n",
    "        if obs not in self.Q: \n",
    "            self.Q[obs] = {}\n",
    "            for act in possible_acts:\n",
    "                self.Q[obs][act] = self.init_val_Q\n",
    "            self.len_Q += 1\n",
    "        \n",
    "        if self.len_Q > self.memory_size:\n",
    "                print(f'The number of registered observations has reached the limit of {self.max_memory:d}')\n",
    "                sys.exit()\n",
    "\n",
    "    def learn(self, rwd, done, env):\n",
    "        \"\"\"学習\"\"\"\n",
    "        if rwd is None:  # rwdがNoneだったら戻る\n",
    "            return\n",
    "        # obs, next_obs を文字列に変換\n",
    "        last_obs = str(self.last_obss[0]) \n",
    "        obs = str(self.last_obss[1])\n",
    "        \n",
    "        possible_acts =  [i * env.size + j for i in range(env.size) for j in range(env.size) if self.last_obss[1][i][j] == 0]\n",
    "        \n",
    "        self._check_and_add_observation(obs, possible_acts)\n",
    "        \n",
    "        # 学習のターゲットを作成\n",
    "        if done is True:\n",
    "            target = rwd\n",
    "        else:\n",
    "            target = rwd + self.gamma * max(self.Q[obs].values())\n",
    "        # Qをターゲットに近づける\n",
    "        self.Q[last_obs][self.last_act] = (1 - self.alpha) * self.Q[last_obs][self.last_act] + self.alpha * target\n",
    "\n",
    "    def get_Q(self, obs):\n",
    "        \"\"\"観測に対するQ値を出力\"\"\"\n",
    "        obs = str(obs)\n",
    "        if obs in self.Q:   # obsがQにある\n",
    "            Q = self.Q[obs]\n",
    "        else:               # obsがQにない\n",
    "            Q = None\n",
    "        return Q\n",
    "    \n",
    "    def save_weights(self):\n",
    "        \"\"\"方策のパラメータの保存\"\"\"\n",
    "        # Qテーブルの保存\n",
    "        filepath = self.filepath + '.pkl'\n",
    "        with open(filepath, mode='wb') as f:\n",
    "            pickle.dump(self.Q, f)\n",
    "\n",
    "    def load_weights(self):\n",
    "        \"\"\"方策のパラメータの読み込み\"\"\"\n",
    "        # Qテーブルの読み込み\n",
    "        filepath = self.filepath + '.pkl'\n",
    "        with open(filepath, mode='rb') as f:\n",
    "            self.Q = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0837a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainQL(agent1, agent2, size=3, nplay=100000):\n",
    "    # 環境の準備\n",
    "    env = TicTacToeEnv(size)  # TicTacToe環境を作成\n",
    "\n",
    "    # 勝敗のカウント\n",
    "    wins_O, wins_X, draws = 0, 0, 0\n",
    "    episode = 0\n",
    "  \n",
    "    running = True\n",
    "    while running and sum([wins_O, wins_X, draws]) != nplay:\n",
    "        # 学習の準備\n",
    "        rwd = 0\n",
    "        \n",
    "        # 各playerごとに処理\n",
    "        if env.player_turn == agent1.my_turn:\n",
    "            act = agent1.act(env)  # agent1が行動を決定\n",
    "            agent1.last_obss.append(deepcopy(env.board))  # 現在のボードを保存\n",
    "            # 状態を更新\n",
    "            done = env.step(act)  # 環境に行動を適用\n",
    "            if done:  # ゲームが終了した場合\n",
    "                if env.winner == 0:\n",
    "                    rwd = 0  # 引き分け\n",
    "                elif env.winner == agent1.my_turn:\n",
    "                    rwd = 1  # agent1の勝利\n",
    "                elif env.winner == agent2.my_turn:\n",
    "                    rwd = -1  # agent2の勝利\n",
    "                agent2.last_obss.append(deepcopy(env.board))  # 最後の状態を保存\n",
    "                agent2.learn(rwd * -1, done, env)  # agent2に報酬を与え学習させる\n",
    "            \n",
    "            # 学習\n",
    "            if len(agent1.last_obss) == 2:\n",
    "                agent1.learn(rwd, done, env)  # agent1が学習\n",
    "            agent1.last_act = act  # 最後の行動を保存\n",
    "            \n",
    "        elif env.player_turn == agent2.my_turn:\n",
    "            act = agent2.act(env)  # agent2が行動を決定\n",
    "            agent2.last_obss.append(deepcopy(env.board))\n",
    "            done = env.step(act)\n",
    "            if done:\n",
    "                if env.winner == 0:\n",
    "                    rwd = 0\n",
    "                elif env.winner == agent2.my_turn:\n",
    "                    rwd = 1\n",
    "                elif env.winner == agent1.my_turn:\n",
    "                    rwd = -1\n",
    "                agent1.last_obss.append(deepcopy(env.board))\n",
    "                agent1.learn(rwd * -1, done, env)\n",
    "            if len(agent2.last_obss) == 2:\n",
    "                agent2.learn(rwd, done, env)\n",
    "            agent2.last_act = act\n",
    "            \n",
    "        # 終了判定\n",
    "        if done:\n",
    "            if env.winner == 1:\n",
    "                wins_O += 1  # Oの勝利をカウント\n",
    "            elif env.winner == -1:\n",
    "                wins_X += 1  # Xの勝利をカウント\n",
    "            else:\n",
    "                draws += 1  # 引き分けをカウント\n",
    "            # 初期化\n",
    "            env.game_reset()  # ゲームをリセット\n",
    "            agent1.last_obss.clear()  # agent1の履歴をクリア\n",
    "            agent2.last_obss.clear()  # agent2の履歴をクリア\n",
    "            episode += 1\n",
    "            \n",
    "            # 10000エピソードごとに進行状況を表示\n",
    "            if episode % 10000 == 0:\n",
    "                print(f\"episode:{episode}/len_Q:{agent1.len_Q} - O wins: {wins_O}, X wins: {wins_X}, Draws: {draws}\")\n",
    "            done = False\n",
    "    \n",
    "    print(f\"Final Results: O wins: {wins_O}, X wins: {wins_X}, Draws: {draws}\")\n",
    "\n",
    "    pygame.quit()  # Pygameを終了する\n",
    "    \n",
    "    # 重みパラメータの保存\n",
    "    agent2.save_weights()  # agent2の重みを保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aede0ecb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent1 = QLAgent(1)\n",
    "agent2 = QLAgent(-1)\n",
    "#trainQL(agent1,agent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cf0e9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner: Draw\r"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "agent1 = HumanAgent(1)\n",
    "agent2 = QLAgent(-1,epsilon=0)\n",
    "agent2.load_weights()\n",
    "progress(agent1,agent2,nplay=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadf7fa2",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76ae295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from collections import deque\n",
    "import random\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class Memory:\n",
    "    \"\"\"経験再生のメモリクラス\"\"\"\n",
    "    def __init__(self, memory_size=100, batch_size=30):\n",
    "        self.memory_size = memory_size\n",
    "        self.buffer = deque(maxlen=memory_size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        # 右側に経験を追加\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # バッチサイズ分の経験をサンプリングする\n",
    "        idx = random.sample(range(len(self.buffer)), batch_size)\n",
    "        return [self.buffer[i] for i in idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, my_turn, size, \n",
    "                 gamma = 0.9, # 割引率\n",
    "                 epsilon = 0.1, # 乱雑度\n",
    "                 memory_size = 1, # 経験の保存数\n",
    "                 batch_size = 1, # 学習で使用する経験の数\n",
    "                 target_interval = 1 # ターゲットを更新する間隔\n",
    "                ):\n",
    "        \n",
    "        # パラメータ\n",
    "        self.my_turn = my_turn\n",
    "        self.input_size = (size,size)\n",
    "        self.n_act = size**2\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon  \n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.target_interval = target_interval\n",
    "        self.model = self._build_Qnet()\n",
    "        self.last_obss = deque(maxlen=2)\n",
    "        self.last_act = None\n",
    "        self.time = 0\n",
    "        self.filepath='agt_data/tictaktoe_DQN'\n",
    "        \n",
    "        # 学習過程の記録関連\n",
    "        self.hist_rwds = []\n",
    "        \n",
    "        # ターゲットモデルの生成\n",
    "        self.model_target = self._build_Qnet()\n",
    "        # メモリのインスタンスを作成\n",
    "        self.memory = Memory(memory_size=self.memory_size, batch_size=batch_size)\n",
    "\n",
    "    def _build_Qnet(self):\n",
    "        # Qネットワークの構築\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=self.input_size))\n",
    "        model.add(Dense(162, activation='relu'))\n",
    "        model.add(Dense(162, activation='relu'))\n",
    "        model.add(Dense(self.n_act, activation='linear'))\n",
    "        \n",
    "        # 勾配法のパラメータの定義\n",
    "        model.compile(loss='mse', optimizer='Adam')\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def act(self, obs):\n",
    "        # 確率でε-greedy法ではない\n",
    "        if random.random() <= self.epsilon:\n",
    "            act = random.randrange(self.n_act)\n",
    "        else:\n",
    "            # Q値を予測する\n",
    "            Q = self.get_Q(obs)\n",
    "            act = Q.index(max(Q))  # 最大となるQ値を出力\n",
    "        return act\n",
    "    \n",
    "    def get_Q(self, obs, type='main'):\n",
    "        # obsを入力し出力を得る\n",
    "        obs_reshaped = np.array([obs])\n",
    "        if type == 'main':\n",
    "            # Qネットワークに観測obsを入力し出力を得る\n",
    "            Q = self.model.predict(obs_reshaped, verbose=0)[0, :]\n",
    "        elif type == 'target':\n",
    "            # ターゲットネットに観測obsを入力し出力を得る\n",
    "            Q = self.model_target.predict(obs_reshaped, verbose=0)[0, :]\n",
    "\n",
    "        return Q.tolist()\n",
    "             \n",
    "\n",
    "    def learn(self, rwd, done):\n",
    "        if rwd is None:\n",
    "            return\n",
    "        \n",
    "        last_obs = self.last_obss[0]\n",
    "        obs = self.last_obss[1]\n",
    "        \n",
    "        self.memory.add((last_obs, self.last_act, rwd, done, obs))\n",
    "        \n",
    "        # 学習\n",
    "        self._fit()\n",
    "\n",
    "        # target_intervalの周期でQネットワークの重みをターゲットネットにコピー\n",
    "        if self.time % self.target_interval == 0 and self.time > 0:\n",
    "            self.model_target.set_weights(self.model.get_weights())\n",
    "        print(self.time, end='\\r')\n",
    "\n",
    "        self.time += 1\n",
    "        \n",
    "    def _fit(self):\n",
    "        # 記憶された「経験」のデータの量がバッチサイズに満たない場合は戻る\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # 学習に使うデータを出力\n",
    "        outs = self.memory.sample(self.batch_size)\n",
    "\n",
    "        # 観測とターゲットのバッチを入れる配列を準備\n",
    "        obs_shape = self.input_size\n",
    "        obss = np.zeros((self.batch_size,) + obs_shape)\n",
    "        targets = np.zeros((self.batch_size, self.n_act))\n",
    "        \n",
    "        for i, (obs, act, rwd, done, next_obs) in enumerate(outs):\n",
    "            # obs に対するQネットワークの出力 yを得る\n",
    "            y = self.get_Q(obs)\n",
    "\n",
    "            # target にyの内容をコピーする\n",
    "            target = y[:]\n",
    "\n",
    "            if not done:\n",
    "                # 最終状態でなかったら next_obsに対する next_yを得る\n",
    "                next_y = self.get_Q(next_obs)\n",
    "\n",
    "                # Q[obs][act]のtarget_act を作成\n",
    "                target_act = rwd + self.gamma * max(next_y)\n",
    "            else:\n",
    "                # 最終状態の場合は報酬だけでtarget_actを作成\n",
    "                target_act = rwd\n",
    "\n",
    "            # targetのactの要素だけtarget_actにする\n",
    "            target[act] = target_act\n",
    "\n",
    "            # obsとtargetをバッチの配列に入れる\n",
    "            obss[i] = obs\n",
    "            targets[i] = target\n",
    "        \n",
    "        # obssと targets のバッチのペアを与えて学習\n",
    "        self.model.fit(obss, targets, verbose=0, epochs=1)\n",
    "\n",
    "    \n",
    "    def save_weights(self):\n",
    "        self.model.save(self.filepath + '.keras', overwrite=True)\n",
    "\n",
    "    def load_weights(self):\n",
    "        # モデルの重みを読み込む\n",
    "        self.model = tf.keras.models.load_model(self.filepath + '.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c14f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainDQN(agent1, agent2, size=3, nplay=100):\n",
    "    # 環境の準備\n",
    "    env = TicTacToeEnv(size)  # TicTacToe環境を作成\n",
    "\n",
    "    # 勝敗のカウント\n",
    "    wins_O, wins_X, draws = 0, 0, 0\n",
    "    episode = 0\n",
    "  \n",
    "    running = True\n",
    "    while running and sum([wins_O, wins_X, draws]) != nplay:\n",
    "        # 学習の準備\n",
    "        rwd = 0\n",
    "        \n",
    "        # 各playerごとに処理\n",
    "        if env.player_turn == agent1.my_turn:\n",
    "            act = agent1.act(env)  # agent1が行動を決定\n",
    "            while env.invalid_move != None:\n",
    "                agent1.learn(rwd=-1, done=False, env=env)\n",
    "                act = agent1.act(env)\n",
    "            agent1.last_obss.append(deepcopy(env.board))  # 現在のボードを保存\n",
    "            # 状態を更新\n",
    "            done = env.step(act)  # 環境に行動を適用\n",
    "            if done:  # ゲームが終了した場合\n",
    "                if env.winner == 0:\n",
    "                    rwd = 0  # 引き分け\n",
    "                elif env.winner == agent1.my_turn:\n",
    "                    rwd = 1  # agent1の勝利\n",
    "                elif env.winner == agent2.my_turn:\n",
    "                    rwd = -1  # agent2の勝利\n",
    "            \n",
    "            # 学習\n",
    "            if len(agent1.last_obss) == 2:\n",
    "                agent1.learn(rwd, done, env)  # agent1が学習\n",
    "            agent1.last_act = act  # 最後の行動を保存\n",
    "            \n",
    "        elif env.player_turn == agent2.my_turn:\n",
    "            act = agent2.act(env)  # agent2が行動を決定\n",
    "            done = env.step(act)\n",
    "            if done:\n",
    "                if env.winner == 0:\n",
    "                    rwd = 0\n",
    "                elif env.winner == agent2.my_turn:\n",
    "                    rwd = 1\n",
    "                elif env.winner == agent1.my_turn:\n",
    "                    rwd = -1\n",
    "                agent1.last_obss.append(deepcopy(env.board))\n",
    "                agent1.learn(rwd * -1, done, env)\n",
    "            \n",
    "        # 終了判定\n",
    "        if done:\n",
    "            if env.winner == 1:\n",
    "                wins_O += 1  # Oの勝利をカウント\n",
    "            elif env.winner == -1:\n",
    "                wins_X += 1  # Xの勝利をカウント\n",
    "            else:\n",
    "                draws += 1  # 引き分けをカウント\n",
    "            # 初期化\n",
    "            env.game_reset()  # ゲームをリセット\n",
    "            agent1.last_obss.clear()  # agent1の履歴をクリア\n",
    "            agent2.last_obss.clear()  # agent2の履歴をクリア\n",
    "            episode += 1\n",
    "            \n",
    "            # 記録\n",
    "            agent1.hist_rwds.append(rwd)\n",
    "            \n",
    "            # 10000エピソードごとに進行状況を表示\n",
    "            if episode % 10000 == 0:\n",
    "                print(f\"episode:{episode}/len_Q:{agent1.len_Q} - O wins: {wins_O}, X wins: {wins_X}, Draws: {draws}\")\n",
    "            done = False\n",
    "    \n",
    "    print(f\"Final Results: O wins: {wins_O}, X wins: {wins_X}, Draws: {draws}\")  \n",
    "    \n",
    "    # 重みパラメータの保存\n",
    "    agent1.save_weights()  # agent1の重みを保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56cfd693",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type TicTacToeEnv).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m agent1 \u001b[38;5;241m=\u001b[39m DQNAgent(\u001b[38;5;241m1\u001b[39m,size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      3\u001b[0m agent2 \u001b[38;5;241m=\u001b[39m QLAgent(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mtrainDQN\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent1\u001b[49m\u001b[43m,\u001b[49m\u001b[43magent2\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[29], line 16\u001b[0m, in \u001b[0;36mtrainDQN\u001b[1;34m(agent1, agent2, size, nplay)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 各playerごとに処理\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env\u001b[38;5;241m.\u001b[39mplayer_turn \u001b[38;5;241m==\u001b[39m agent1\u001b[38;5;241m.\u001b[39mmy_turn:\n\u001b[1;32m---> 16\u001b[0m     act \u001b[38;5;241m=\u001b[39m \u001b[43magent1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# agent1が行動を決定\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m env\u001b[38;5;241m.\u001b[39minvalid_move \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m         agent1\u001b[38;5;241m.\u001b[39mlearn(rwd\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, done\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, env\u001b[38;5;241m=\u001b[39menv)\n",
      "Cell \u001b[1;32mIn[28], line 78\u001b[0m, in \u001b[0;36mDQNAgent.act\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     75\u001b[0m     act \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandrange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_act)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;66;03m# Q値を予測する\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m     Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_Q\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     act \u001b[38;5;241m=\u001b[39m Q\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mmax\u001b[39m(Q))  \u001b[38;5;66;03m# 最大となるQ値を出力\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m act\n",
      "Cell \u001b[1;32mIn[28], line 87\u001b[0m, in \u001b[0;36mDQNAgent.get_Q\u001b[1;34m(self, obs, type)\u001b[0m\n\u001b[0;32m     84\u001b[0m obs_reshaped \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([obs])\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# Qネットワークに観測obsを入力し出力を得る\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m     Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m, :]\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;66;03m# ターゲットネットに観測obsを入力し出力を得る\u001b[39;00m\n\u001b[0;32m     90\u001b[0m     Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_target\u001b[38;5;241m.\u001b[39mpredict(obs_reshaped, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type TicTacToeEnv)."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    agent1 = DQNAgent(1,size=3)\n",
    "    agent2 = QLAgent(-1)\n",
    "    \n",
    "    trainDQN(agent1,agent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f184fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
