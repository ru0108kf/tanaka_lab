{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8de72e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pygame\n",
    "import random\n",
    "import sys\n",
    "\n",
    "class Marubatsu:\n",
    "    def __init__(\n",
    "            self,\n",
    "            size=3, # ボードのサイズ\n",
    "            current_player = random.choice([1, -1])# ランダムに先攻を決定\n",
    "            ):\n",
    "        self.size = size \n",
    "        self.board = np.array([[0]*size for _ in range(size)]) # サイズをもとにボードを作成\n",
    "        self.current_player = current_player  \n",
    "        self.winner = 0 # 勝利者\n",
    "        self.reward = 0# 報酬\n",
    "    \n",
    "    def game_reset(self):\n",
    "        \"\"\"ボードの初期化\"\"\"\n",
    "        self.board = np.array([[0]*self.size for _ in range(self.size)])\n",
    "        self.current_player = random.choice([1, -1])  \n",
    "        self.winner = 0\n",
    "        self.reward = 0\n",
    "\n",
    "    def done(self):\n",
    "        \"\"\"終了判定\"\"\"\n",
    "        # 行と列のチェック\n",
    "        player = self.current_player*-1\n",
    "        for i in range(self.size):\n",
    "            if np.all(self.board[i, :] == player) or np.all(self.board[:, i] == player):\n",
    "                self.winner = player\n",
    "                self.reward += 1 if player == 1 else -1\n",
    "                return True\n",
    "        \n",
    "        # 対角線のチェック\n",
    "        if np.all(np.diag(self.board) == player) or np.all(np.diag(np.fliplr(self.board)) == player):\n",
    "            self.winner = player\n",
    "            self.reward += 1 if player == 1 else -1\n",
    "            return True\n",
    "\n",
    "        # ドローのチェック\n",
    "        if not np.any(self.board == 0):\n",
    "            self.winner = 0\n",
    "            self.reward += 0.3\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def step(self, act):\n",
    "        \"\"\"状態を更新\"\"\"\n",
    "        x, y = divmod(act, self.size)\n",
    "        # actを受け取って、次の状態にする\n",
    "        if self.board[x, y] == 0: # 受け取ったactが有効なら\n",
    "            x, y = divmod(act, self.size)\n",
    "            self.board[x, y] = self.current_player\n",
    "            self.reward -= 0.01\n",
    "            self.current_player *= -1\n",
    "        else: # 無効ならペナルティーを与える\n",
    "            self.reward -= 0.1\n",
    "\n",
    "    def pygame_init(self):\n",
    "        \"\"\"pygame開始\"\"\"\n",
    "        # pygameを開始する\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((self.size * 100, self.size * 100))\n",
    "        self.font = pygame.font.Font(None, 100)\n",
    "        pygame.display.set_caption(\"Marubatsu Game\")\n",
    "        self.pygame_render(self.board)\n",
    "\n",
    "    def pygame_render(self, board):\n",
    "        \"\"\"描画関数\"\"\"\n",
    "        # pygameでboardの内容を描画する\n",
    "        WHITE = (255, 255, 255)\n",
    "        BLACK = (0, 0, 0)\n",
    "        self.screen.fill(WHITE)\n",
    "        for x in range(1, self.size):\n",
    "            pygame.draw.line(self.screen, BLACK, (x * 100, 0), (x * 100, self.size * 100), 3)\n",
    "            pygame.draw.line(self.screen, BLACK, (0, x * 100), (self.size * 100, x * 100), 3)\n",
    "        \n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                if board[i, j] == 1:\n",
    "                    text = self.font.render('X', True, BLACK)\n",
    "                    self.screen.blit(text, (j * 100 + 25, i * 100 + 15))\n",
    "                elif board[i, j] == -1:\n",
    "                    text = self.font.render('O', True, BLACK)\n",
    "                    self.screen.blit(text, (j * 100 + 25, i * 100 + 15))\n",
    "\n",
    "        pygame.display.flip()\n",
    "\n",
    "class RandomAgent:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def act(self, board):\n",
    "        available_moves = np.argwhere(board == 0)\n",
    "        move = random.choice(available_moves)\n",
    "        act = move[0] * self.size + move[1]\n",
    "        return act\n",
    "\n",
    "class PlayerAgent:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def act(self, board):\n",
    "        while True:\n",
    "            for event in pygame.event.get():  # Pygameのイベントを処理する\n",
    "                if event.type == pygame.MOUSEBUTTONDOWN and event.button == 1:\n",
    "                    pos = pygame.mouse.get_pos()  # マウスの位置を取得する\n",
    "                    x, y = pos[1] // 100, pos[0] // 100  # マウスの位置をボードのセルに変換する\n",
    "                    act = x * self.size + y  # 行と列を1次元のインデックスに変換する\n",
    "                    if board[x, y] == 0:\n",
    "                        return act\n",
    "                elif event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "76ae295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from collections import deque\n",
    "import csv\n",
    "import os\n",
    "\n",
    "class Memory:\n",
    "    \"\"\"経験再生のメモリクラス\"\"\"\n",
    "    def __init__(self, memory_size=100, batch_size=30):\n",
    "        self.memory_size = memory_size\n",
    "        self.buffer = deque(maxlen=batch_size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        # 右側に経験を追加\n",
    "        self.buffer.append(experience)\n",
    "        # 保存上限に達したら、左側の経験を削除\n",
    "        if len(self.buffer) > self.memory_size:\n",
    "            self.buffer.popleft()\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # バッチサイズ分の経験をサンプリングする\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)\n",
    "        return [self.buffer[i] for i in idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, size, \n",
    "                 gamma = 0.9, # 割引率\n",
    "                 epsilon = 0.1, # 乱雑度\n",
    "                 memory_size = 20, # 経験の保存数\n",
    "                 batch_size = 5, # 学習で使用する経験の数\n",
    "                 target_interval = 30 # ターゲットを更新する間隔\n",
    "                ):\n",
    "        \n",
    "        # パラメータ\n",
    "        self.input_size = (size,size)\n",
    "        self.n_act = size**2\n",
    "        self.gamma = gamma \n",
    "        self.epsilon = epsilon  \n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.target_interval = target_interval\n",
    "        self.model = self._build_Qnet()\n",
    "        self.time = 0\n",
    "        \n",
    "        # 学習過程の記録関連\n",
    "        self.hist_rwds = []\n",
    "        self.hist_wnrs = []\n",
    "        \n",
    "        # ターゲットモデルの生成\n",
    "        self.model_target = self._build_Qnet()\n",
    "        # メモリのインスタンスを作成\n",
    "        self.memory = Memory(memory_size=self.memory_size, batch_size=batch_size)\n",
    "\n",
    "    def _build_Qnet(self):\n",
    "        # Qネットワークの構築\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=self.input_size))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(self.n_act, activation='linear'))\n",
    "        \n",
    "        # 勾配法のパラメータの定義\n",
    "        model.compile(loss='mse', optimizer='Adam')\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def act(self, obs):\n",
    "        # 確率でε-greedy法ではない\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            act = random.randrange(self.n_act)\n",
    "        else:\n",
    "            # Q値を予測する\n",
    "            Q = self.get_Q(obs)\n",
    "            act = np.argmax(Q) # 最大となるQ値を出力\n",
    "        return act\n",
    "    \n",
    "    def get_Q(self, obs, type='main'):\n",
    "        # obsを入力し出力を得る\n",
    "        if type == 'main':\n",
    "            # Qネットワークに観測obsを入力し出力を得る\n",
    "            Q = self.model.predict(obs.reshape((1,) + self.input_size), verbose=0)[0, :]\n",
    "        elif type == 'target':\n",
    "            # ターゲットネットに観測obsを入力し出力を得る\n",
    "            Q = self.model_target.predict(obs.reshape((1,) + self.input_size), verbose=0)[0, :]\n",
    "\n",
    "        return Q\n",
    "             \n",
    "\n",
    "    def learn(self, obs, act, rwd, done, next_obs):\n",
    "        if rwd is None:\n",
    "            return\n",
    "        \n",
    "        self.memory.add((obs, act, rwd, done, next_obs))\n",
    "        \n",
    "        # 学習\n",
    "        self._fit()\n",
    "\n",
    "        # target_intervalの周期でQネットワークの重みをターゲットネットにコピー\n",
    "        if self.time % self.target_interval == 0 and self.time > 0:\n",
    "            self.model_target.set_weights(self.model.get_weights())\n",
    "\n",
    "        self.time += 1\n",
    "        \n",
    "    def _fit(self):\n",
    "        # 記憶された「経験」のデータの量がバッチサイズに満たない場合は戻る\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # 学習に使うデータを出力\n",
    "        outs = self.memory.sample(self.batch_size)\n",
    "\n",
    "        # 観測とターゲットのバッチを入れる配列を準備\n",
    "        obs = outs[0][0]  # 1番目の経験の観測を利用して、配列を作る\n",
    "        obss = np.zeros((self.batch_size,) + obs.shape,dtype=int)\n",
    "        targets = np.zeros((self.batch_size, self.n_act))\n",
    "        \n",
    "        for i, out in enumerate(outs):\n",
    "            # 経験を要素に分解\n",
    "            obs, act, rwd, done, next_obs = out\n",
    "\n",
    "            # obs に対するQネットワークの出力 yを得る\n",
    "            y = self.get_Q(obs)\n",
    "\n",
    "            # target にyの内容をコピーする\n",
    "            target = y.copy()\n",
    "\n",
    "            if done is False:\n",
    "                # 最終状態でなかったら next_obsに対する next_yを得る\n",
    "                next_y = self.get_Q(next_obs)\n",
    "\n",
    "                # Q[obs][act]のtarget_act を作成\n",
    "                target_act = rwd + self.gamma * max(next_y)\n",
    "            else:\n",
    "                # 最終状態の場合は報酬だけでtarget_actを作成\n",
    "                target_act = rwd\n",
    "\n",
    "            # targetのactの要素だけtarget_actにする\n",
    "            target[act] = target_act\n",
    "\n",
    "            # obsとtargetをバッチの配列に入れる\n",
    "            obss[i, :] = obs\n",
    "            targets[i, :] = target\n",
    "        \n",
    "        # obssと targets のバッチのペアを与えて学習\n",
    "        self.model.fit(obss, targets, verbose=0, epochs=1)\n",
    "\n",
    "    \n",
    "    def save_weights(self, filepath='agt_data/noname'):\n",
    "        self.model.save(filepath + '.keras', overwrite=True)\n",
    "        \n",
    "        # episodeとrwdをCSVファイルに保存\n",
    "        with open(filepath + '.csv', \"w\", newline='') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            writer.writerow(['episode','reward','winner'])\n",
    "            for i, (rwd, wnr) in enumerate(zip(self.hist_rwds, self.hist_wnrs)):\n",
    "                writer.writerow([i+1, rwd, wnr])\n",
    "\n",
    "    def load_weights(self, filepath='agt_data/noname'):\n",
    "        # モデルの重みを読み込む\n",
    "        self.model = tf.keras.models.load_model(filepath + '.keras')\n",
    "        \n",
    "        # episodeとrwdをCSVファイルから読み込む\n",
    "        with open(filepath + '.csv', \"r\") as csv_file:\n",
    "            reader = csv.reader(csv_file)\n",
    "            next(reader)\n",
    "            for row in reader:\n",
    "                self.hist_rwds.append(float(row[1]))\n",
    "                self.hist_wnrs.append(float(row[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2c14f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    env = Marubatsu()\n",
    "\n",
    "    agent1 = DQNAgent(env.size)\n",
    "    agent2 = RandomAgent(env.size)\n",
    "    \n",
    "    filepath='agt_data/tictaktoe_DQN'\n",
    "    \n",
    "    episode = 1\n",
    "    \n",
    "    if os.path.exists(filepath + '.keras'):\n",
    "        agent1.load_weights(filepath)\n",
    "        episode += len(agent1.hist_rwds)\n",
    "    \n",
    "    env.pygame_init()\n",
    "    running = True\n",
    "    \n",
    "    while running:\n",
    "        obs = env.board\n",
    "        env.pygame_render(obs)\n",
    "        \n",
    "        if env.current_player == 1:\n",
    "            act = agent1.act(obs)\n",
    "        elif env.current_player == -1:\n",
    "            act = agent2.act(obs)\n",
    "\n",
    "        # 状態を更新\n",
    "        env.step(act)\n",
    "        next_obs = env.board\n",
    "        done = env.done()\n",
    "        rwd = round(env.reward,2)\n",
    "        \n",
    "        # 描画\n",
    "        env.pygame_render(obs)\n",
    "        \n",
    "        # 学習\n",
    "        agent1.learn(obs, act, rwd, done, next_obs)\n",
    "        \n",
    "        if done:\n",
    "            # 記録\n",
    "            agent1.hist_rwds.append(rwd)\n",
    "            agent1.hist_wnrs.append(env.winner)\n",
    "\n",
    "            # 勝率\n",
    "            win = (agent1.hist_wnrs.count(1) / episode) * 100\n",
    "\n",
    "            # 結果を表示\n",
    "            print(f\"episode{episode}/Reward: {rwd}/Win %: {win} \")\n",
    "            # 初期化\n",
    "            env.game_reset()\n",
    "            episode += 1\n",
    "        \n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "        \n",
    "        if episode > 50000:\n",
    "            running = False\n",
    "\n",
    "    pygame.quit()\n",
    "    \n",
    "    # 重みパラメータの保存\n",
    "    agent1.save_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "56cfd693",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode6/Reward: -3.18/Win %: 33.33333333333333 \n",
      "episode7/Reward: -1.07/Win %: 28.57142857142857 \n",
      "episode8/Reward: -3.79/Win %: 25.0 \n",
      "episode9/Reward: -2.99/Win %: 33.33333333333333 \n",
      "episode10/Reward: -1.59/Win %: 40.0 \n",
      "episode11/Reward: -1.25/Win %: 36.36363636363637 \n",
      "episode12/Reward: -1.97/Win %: 33.33333333333333 \n",
      "episode13/Reward: -1.68/Win %: 38.46153846153847 \n",
      "episode14/Reward: -3.69/Win %: 35.714285714285715 \n",
      "episode15/Reward: -2.89/Win %: 33.33333333333333 \n",
      "episode16/Reward: -0.58/Win %: 37.5 \n",
      "episode17/Reward: -2.96/Win %: 35.294117647058826 \n",
      "episode18/Reward: -5.09/Win %: 33.33333333333333 \n",
      "episode19/Reward: -3.25/Win %: 31.57894736842105 \n",
      "episode20/Reward: -1.67/Win %: 35.0 \n",
      "episode21/Reward: -3.19/Win %: 33.33333333333333 \n",
      "episode22/Reward: -4.48/Win %: 31.818181818181817 \n",
      "episode23/Reward: -2.98/Win %: 34.78260869565217 \n",
      "episode24/Reward: -1.15/Win %: 33.33333333333333 \n",
      "episode25/Reward: -1.07/Win %: 32.0 \n",
      "episode26/Reward: 0.55/Win %: 34.61538461538461 \n",
      "episode27/Reward: -4.19/Win %: 33.33333333333333 \n",
      "episode28/Reward: -3.38/Win %: 35.714285714285715 \n",
      "episode29/Reward: -4.78/Win %: 34.48275862068966 \n",
      "episode30/Reward: -1.19/Win %: 33.33333333333333 \n",
      "episode31/Reward: -2.27/Win %: 35.483870967741936 \n",
      "episode32/Reward: -3.39/Win %: 37.5 \n",
      "episode33/Reward: -3.49/Win %: 39.39393939393939 \n",
      "episode34/Reward: 0.85/Win %: 41.17647058823529 \n",
      "episode35/Reward: -3.38/Win %: 40.0 \n",
      "episode36/Reward: -3.67/Win %: 38.88888888888889 \n",
      "episode37/Reward: -0.45/Win %: 40.54054054054054 \n",
      "episode38/Reward: -3.99/Win %: 42.10526315789473 \n",
      "episode39/Reward: -4.47/Win %: 43.58974358974359 \n",
      "episode40/Reward: -2.96/Win %: 45.0 \n",
      "episode41/Reward: -4.38/Win %: 43.90243902439025 \n",
      "episode42/Reward: -3.56/Win %: 42.857142857142854 \n",
      "episode43/Reward: -6.19/Win %: 41.86046511627907 \n",
      "episode44/Reward: -7.39/Win %: 40.909090909090914 \n",
      "episode45/Reward: -1.27/Win %: 40.0 \n",
      "episode46/Reward: -4.07/Win %: 39.130434782608695 \n",
      "episode47/Reward: -5.19/Win %: 38.297872340425535 \n",
      "episode48/Reward: -6.19/Win %: 37.5 \n",
      "episode49/Reward: -3.77/Win %: 38.775510204081634 \n",
      "episode50/Reward: -6.09/Win %: 38.0 \n",
      "episode51/Reward: 0.95/Win %: 39.21568627450981 \n",
      "episode52/Reward: -3.97/Win %: 40.38461538461539 \n",
      "episode53/Reward: -1.86/Win %: 41.509433962264154 \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "47a70e11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "Winner: O\n",
      "Reward: -1.05\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "Winner: X\n",
      "Reward: 0.95\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = Marubatsu(3)\n",
    "    \n",
    "    agent1 = PlayerAgent(env.size)\n",
    "    agent2 = PlayerAgent(env.size)\n",
    "    \n",
    "    env.pygame_init()  # Pygameを初期化し、ゲームウィンドウを設定する\n",
    "    running = True\n",
    "    \n",
    "    while running:\n",
    "        env.pygame_render(env.board)\n",
    "        \n",
    "        # 行動を決める\n",
    "        if env.current_player == 1:\n",
    "            act = agent1.act(env.board)\n",
    "        elif env.current_player == -1:\n",
    "            act = agent2.act(env.board)\n",
    "        \n",
    "        env.step(act)\n",
    "        env.pygame_render(env.board)\n",
    "        \n",
    "        print(env.current_player)\n",
    "        # 終了判定 \n",
    "        if env.done():\n",
    "            print(f\"Winner: {'X' if env.winner == 1 else 'O' if env.winner == -1 else 'Draw'}\")\n",
    "            print(f\"Reward: {env.reward}\")\n",
    "            # 初期化\n",
    "            env.game_reset()\n",
    "        \n",
    "        # 無効な行動を繰り返してしまった場合\n",
    "        if env.reward <= -3:\n",
    "            print(\"Reward: threshold reached\")\n",
    "            env.game_reset()\n",
    "\n",
    "        for event in pygame.event.get():  # Pygameのイベントを処理する\n",
    "            if event.type == pygame.QUIT:  # ウィンドウの閉じるボタンがクリックされたとき\n",
    "                running = False  # メインループを終了する\n",
    "\n",
    "    pygame.quit()  # Pygameを終了する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bccf05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141eaa65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
