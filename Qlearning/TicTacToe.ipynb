{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de72e4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.10.14)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "import sys\n",
    "\n",
    "class TicTacToe:\n",
    "    def __init__(self, size=3, current_player=random.choice([1, -1])):\n",
    "        self.size = size \n",
    "        self.board = [[0]*size for _ in range(size)]  # サイズをもとにボードを作成\n",
    "        self.current_player = current_player  \n",
    "        self.winner = 0  # 勝利プレイヤー\n",
    "        self.invalid_move = None  # 無効な動作を記録\n",
    "        \n",
    "        \"\"\"\n",
    "        player_O = 1\n",
    "        player_X = -1\n",
    "        \"\"\"\n",
    "        \n",
    "    def game_reset(self):\n",
    "        \"\"\"ボードの初期化\"\"\"\n",
    "        self.board = [[0]*self.size for _ in range(self.size)]\n",
    "        self.current_player = random.choice([1, -1])  \n",
    "        self.winner = 0\n",
    "        self.reward = 0\n",
    "        self.invalid_move = None\n",
    "    \n",
    "    def done(self):\n",
    "        \"\"\"終了判定\"\"\"\n",
    "        player = self.current_player*-1\n",
    "        # 行と列のチェック\n",
    "        for i in range(self.size):\n",
    "            if all(self.board[i][j] == player for j in range(self.size)) or all(self.board[j][i] == player for j in range(self.size)):\n",
    "                self.winner = player\n",
    "                return True\n",
    "        \n",
    "        # 対角線のチェック\n",
    "        if all(self.board[i][i] == player for i in range(self.size)) or all(self.board[i][self.size-i-1] == player for i in range(self.size)):\n",
    "            self.winner = player\n",
    "            return True\n",
    "\n",
    "        # ドローのチェック\n",
    "        if not any(self.board[i][j] == 0 for i in range(self.size) for j in range(self.size)):\n",
    "            self.winner = 0\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def step(self, act):\n",
    "        \"\"\"状態を更新\"\"\"\n",
    "        x, y = divmod(act, self.size)\n",
    "        # actを受け取って、次の状態にする\n",
    "        if self.board[x][y] == 0:  # 受け取ったactが有効なら\n",
    "            self.board[x][y] = self.current_player\n",
    "            self.invalid_move = None\n",
    "            # プレイヤー交代\n",
    "            self.current_player *= -1\n",
    "        else:\n",
    "            self.invalid_move = (x, y)        \n",
    "    \n",
    "    def pygame_init(self):\n",
    "        \"\"\"pygame開始\"\"\"\n",
    "        # pygameを開始する\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((self.size * 100, self.size * 100))\n",
    "        self.font = pygame.font.Font(None, 100)\n",
    "        pygame.display.set_caption(\"Tic Tac Toe\")\n",
    "        self.pygame_render(self.board)\n",
    "\n",
    "    def pygame_render(self, board):\n",
    "        \"\"\"描画関数\"\"\"\n",
    "        # pygameでboardの内容を描画する\n",
    "        WHITE = (255, 255, 255)\n",
    "        BLACK = (0, 0, 0)\n",
    "        RED = (255, 0, 0, 150)\n",
    "        self.screen.fill(WHITE)\n",
    "        for x in range(1, self.size):\n",
    "            pygame.draw.line(self.screen, BLACK, (x * 100, 0), (x * 100, self.size * 100), 3)\n",
    "            pygame.draw.line(self.screen, BLACK, (0, x * 100), (self.size * 100, x * 100), 3)\n",
    "            \n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                if board[i][j] == 1:\n",
    "                    text = self.font.render('O', True, BLACK)\n",
    "                    self.screen.blit(text, (j * 100 + 25, i * 100 + 15))\n",
    "                elif board[i][j] == -1:\n",
    "                    text = self.font.render('X', True, BLACK)\n",
    "                    self.screen.blit(text, (j * 100 + 25, i * 100 + 15))\n",
    "            \n",
    "        if self.invalid_move is not None: \n",
    "            (i, j) = self.invalid_move\n",
    "            pygame.draw.rect(self.screen, RED, (j * 100, i * 100, 100, 100), 3)\n",
    "        \n",
    "        pygame.display.flip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47a70e11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def progress(agent1, agent2, size=3, render=True):\n",
    "    \"\"\"ゲーム進行\"\"\"\n",
    "    env = TicTacToe(size)\n",
    "    \n",
    "    if render:    \n",
    "        env.pygame_init()  # Pygameを初期化し、ゲームウィンドウを設定する\n",
    "    running = True\n",
    "    \n",
    "    while running:\n",
    "        if render:\n",
    "            env.pygame_render(env.board)\n",
    "        # 行動を決める\n",
    "        if env.current_player == 1:\n",
    "            act = agent1.act(env.board)\n",
    "        elif env.current_player == -1:\n",
    "            act = agent2.act(env.board)\n",
    "        \n",
    "        env.step(act)\n",
    "        \n",
    "        if render:\n",
    "            env.pygame_render(env.board)\n",
    "        \n",
    "        # 終了判定 \n",
    "        if env.done():\n",
    "            print(f\"Winner: {'O' if env.winner == 1 else 'X' if env.winner == -1 else 'Draw'}\")\n",
    "            # 初期化\n",
    "            env.game_reset()\n",
    "            \n",
    "        if render:\n",
    "            for event in pygame.event.get():  # Pygameのイベントを処理する\n",
    "                if event.type == pygame.QUIT:  # ウィンドウの閉じるボタンがクリックされたとき\n",
    "                    running = False  # メインループを終了する\n",
    "\n",
    "    pygame.quit()  # Pygameを終了する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2926050",
   "metadata": {},
   "source": [
    "# ランダムとランダムαと人間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07c1637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \"\"\"完全ランダム\"\"\"\n",
    "    def __init__(self, size, my_turn):\n",
    "        self.size = size\n",
    "        self.my_turn = my_turn\n",
    "\n",
    "    def act(self, board):\n",
    "        available_moves = [(i, j) for i in range(self.size) for j in range(self.size) if board[i][j] == 0]\n",
    "        move = random.choice(available_moves)\n",
    "        act = move[0] * self.size + move[1]\n",
    "        return act\n",
    "\n",
    "class RandomalfaAgent:\n",
    "    \"\"\"勝てるところがあれば勝つランダム\"\"\"\n",
    "    def __init__(self, size, my_turn):\n",
    "        self.size = size\n",
    "        self.my_turn = my_turn\n",
    "        \n",
    "    def act(self, board):\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                if board[i][j] == 0:\n",
    "                    board[i][j] = self.my_turn\n",
    "                    if self.check_win(board):\n",
    "                        board[i][j] = 0\n",
    "                        return i * self.size + j\n",
    "                    board[i][j] = 0\n",
    "        \n",
    "        available_moves = [(i, j) for i in range(self.size) for j in range(self.size) if board[i][j] == 0]\n",
    "        move = random.choice(available_moves)\n",
    "        act = move[0] * self.size + move[1]\n",
    "        return act\n",
    "\n",
    "    def check_win(self, board):\n",
    "        for i in range(self.size):\n",
    "            if all(board[i][j] == self.my_turn for j in range(self.size)) or all(board[j][i] == self.my_turn for j in range(self.size)):\n",
    "                return True\n",
    "        \n",
    "        if all(board[i][i] == self.my_turn for i in range(self.size)) or all(board[i][self.size-i-1] == self.my_turn for i in range(self.size)):\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "class HumanAgent:\n",
    "    \"\"\"人が動かすクラス\"\"\"\n",
    "    def __init__(self, size, my_turn):\n",
    "        self.size = size\n",
    "        self.my_turn = my_turn\n",
    "\n",
    "    def act(self, board):\n",
    "        while True:\n",
    "            for event in pygame.event.get():  # Pygameのイベントを処理する\n",
    "                if event.type == pygame.MOUSEBUTTONDOWN and event.button == 1:\n",
    "                    pos = pygame.mouse.get_pos()  # マウスの位置を取得する\n",
    "                    x, y = pos[1] // 100, pos[0] // 100  # マウスの位置をボードのセルに変換する\n",
    "                    act = x * self.size + y  # 行と列を1次元のインデックスに変換する\n",
    "                    if board[x][y] == 0:\n",
    "                        return act\n",
    "                elif event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    sys.exit()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bbc90269",
   "metadata": {},
   "source": [
    "size = 3\n",
    "agent1 = RandomalfaAgent(size, 1)\n",
    "agent2 = RandomalfaAgent(size,-1)\n",
    "progress(agent1,agent2,render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfd5fb5",
   "metadata": {},
   "source": [
    "# モンテカルロ法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "843835ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class MCAgent:\n",
    "    \"\"\"モンテカルロ法\"\"\"\n",
    "    def __init__(self, size, my_turn):\n",
    "        self.size = size\n",
    "        self.my_turn = my_turn\n",
    "\n",
    "    def win_or_rand(self, board, turn):\n",
    "        \"\"\"次の手で勝てる場合、勝ち、そうでないならランダムに動く\"\"\"\n",
    "        available_moves = [(i, j) for i in range(self.size) for j in range(self.size) if board[i][j] == 0]\n",
    "        for move in available_moves:\n",
    "            tempboard = [row[:] for row in board]\n",
    "            tempboard[move[0]][move[1]] = turn\n",
    "            if self.check_winner(tempboard, turn):\n",
    "                return move[0] * self.size + move[1]\n",
    "        move = random.choice(available_moves)\n",
    "        return move[0] * self.size + move[1]\n",
    "\n",
    "    def check_winner(self, board, player):\n",
    "        for i in range(self.size):\n",
    "            if all(board[i][j] == player for j in range(self.size)) or all(board[j][i] == player for j in range(self.size)):\n",
    "                return True\n",
    "        if all(board[i][i] == player for i in range(self.size)) or all(board[i][self.size-i-1] == player for i in range(self.size)):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def trial(self, score, board, act):\n",
    "        tempboard = [row[:] for row in board]\n",
    "        x, y = divmod(act, self.size)\n",
    "        tempboard[x][y] = self.my_turn\n",
    "        temp_turn = self.my_turn\n",
    "        while True:\n",
    "            temp_turn *= -1\n",
    "            if self.check_winner(tempboard, temp_turn):\n",
    "                if temp_turn == self.my_turn:\n",
    "                    score[act] += 1\n",
    "                else:\n",
    "                    score[act] -= 1\n",
    "                break\n",
    "            if not any(tempboard[i][j] == 0 for i in range(self.size) for j in range(self.size)):\n",
    "                break\n",
    "            next_move = self.win_or_rand(tempboard, temp_turn)\n",
    "            x, y = divmod(next_move, self.size)\n",
    "            tempboard[x][y] = temp_turn\n",
    "\n",
    "    def act(self, board):\n",
    "        acts = [i * self.size + j for i in range(self.size) for j in range(self.size) if board[i][j] == 0]\n",
    "        scores = {act: 0 for act in acts}\n",
    "        n = 2\n",
    "        for act in acts:\n",
    "            for _ in range(n):\n",
    "                self.trial(scores, board, act)\n",
    "            scores[act] /= n\n",
    "        return max(scores, key=scores.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd86640e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner: Draw\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "size = 3\n",
    "agent1 = HumanAgent(size, 1)\n",
    "agent2 = MCAgent(size,-1)\n",
    "progress(agent1,agent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca0ab96",
   "metadata": {},
   "source": [
    "# QLarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bae44b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class QLAgent:\n",
    "    def __init__(self, size, \n",
    "                 gamma=0.9,  # 割引率\n",
    "                 epsilon=0.2,  # 乱雑度\n",
    "                 alpha=0.3,  # 学習率\n",
    "                 memory_size=10000):\n",
    "        \n",
    "        # パラメータ\n",
    "        self.input_size = (size, size)\n",
    "        self.n_act = size**2\n",
    "        self.gamma = gamma \n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon  \n",
    "        self.init_val_Q = 0\n",
    "        self.memory_size = memory_size\n",
    "        \n",
    "\n",
    "        # Qテーブル関連\n",
    "        self.Q = {}     # Qテーブル\n",
    "        self.len_Q = 0  # Qテーブルに登録した観測の数\n",
    "        self.episode = 0\n",
    "\n",
    "    def act(self, obs):\n",
    "        \"\"\"観測に対して行動を出力\"\"\"\n",
    "        # obsを文字列に変換\n",
    "        obs = str(obs)\n",
    "\n",
    "        # obs が登録されていなかったら初期値を与えて登録\n",
    "        self._check_and_add_observation(obs)\n",
    "\n",
    "        # 可能な行動を取得\n",
    "        available_moves = self.get_possible_moves(obs)\n",
    "        \n",
    "        # 確率的に処理を分岐\n",
    "        if np.random.rand() < (self.epsilon/(self.episode//10000+1)):\n",
    "            # epsilon の確率\n",
    "            act = random.choice(available_moves)  # ランダム行動\n",
    "        else:\n",
    "            # 1-epsilon の確率\n",
    "            q_values = [self.Q[obs][move] for move in available_moves]\n",
    "            max_q = max(q_values)\n",
    "            max_q_moves = [move for move in available_moves if self.Q[obs][move] == max_q]\n",
    "            act = random.choice(max_q_moves)  # Qを最大にする行動\n",
    "        return act\n",
    "\n",
    "    def get_possible_moves(self, obs):\n",
    "        \"\"\"可能な行動を取得\"\"\"\n",
    "        obs_array = np.array(eval(obs))\n",
    "        return [i * self.input_size[0] + j for i in range(self.input_size[0]) for j in range(self.input_size[1]) if obs_array[i, j] == 0]\n",
    "\n",
    "    def _check_and_add_observation(self, obs):\n",
    "        \"\"\"obs が登録されていなかったら初期値を与えて登録\"\"\"\n",
    "        if obs not in self.Q: \n",
    "            self.Q[obs] = [self.init_val_Q] * self.n_act\n",
    "            self.len_Q += 1\n",
    "\n",
    "    def learn(self, obs, act, rwd, done, next_obs):\n",
    "        \"\"\"学習\"\"\"\n",
    "        if rwd is None:  # rwdがNoneだったら戻る\n",
    "            return\n",
    "        # obs, next_obs を文字列に変換\n",
    "        obs = str(obs)\n",
    "        next_obs = str(next_obs)\n",
    "\n",
    "        # next_obs が登録されていなかったら初期値を与えて登録\n",
    "        self._check_and_add_observation(next_obs)\n",
    "\n",
    "        # 学習のターゲットを作成\n",
    "        if done:\n",
    "            target = rwd\n",
    "            \n",
    "        else:\n",
    "            target = rwd + self.gamma * max(self.Q[next_obs])\n",
    "\n",
    "        # Qをターゲットに近づける\n",
    "        self.Q[obs][act] = (1 - self.alpha) * self.Q[obs][act] + self.alpha * target\n",
    "\n",
    "    def get_Q(self, obs):\n",
    "        \"\"\"観測に対するQ値を出力\"\"\"\n",
    "        obs = str(obs)\n",
    "        if obs in self.Q:   # obsがQにある\n",
    "            val = self.Q[obs]\n",
    "            Q = np.array(val)\n",
    "        else:               # obsがQにない\n",
    "            Q = None\n",
    "        return Q\n",
    "    \n",
    "    def save_weights(self,filepath='agt_data/noname'):\n",
    "        \"\"\"方策のパラメータの保存\"\"\"\n",
    "        # Qテーブルの保存\n",
    "        filepath = filepath + '.pkl'\n",
    "        with open(filepath, mode='wb') as f:\n",
    "            pickle.dump(self.Q, f)\n",
    "\n",
    "    def load_weights(self,filepath='agt_data/noname'):\n",
    "        \"\"\"方策のパラメータの読み込み\"\"\"\n",
    "        # Qテーブルの読み込み\n",
    "        filepath = filepath + '.pkl'\n",
    "        with open(filepath, mode='rb') as f:\n",
    "            self.Q = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0837a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def trainQL():\n",
    "    env = TicTacToe()\n",
    "\n",
    "    agent1 = QLAgent(env.size,1)\n",
    "    agent2 = QLAgent(env.size,-1)\n",
    "    \n",
    "    filepath='agt_data/tictactoe_QL'\n",
    "    \n",
    "    agent2.load_weights(filepath)\n",
    "    agent2.len_Q = len(agent2.Q)\n",
    "    \n",
    "    running = True\n",
    "    \n",
    "    while running:\n",
    "        obs = deepcopy(env.board)\n",
    "        rwd = 0\n",
    "        \n",
    "        if env.current_player == 1:\n",
    "            act = agent1.act(obs)\n",
    "            # 状態を更新\n",
    "            env.step(act)\n",
    "            next_obs = env.board\n",
    "            done = env.done()\n",
    "            # 勝者が1の時\n",
    "            rwd = 0 if env.winner == 0 else (1 if env.winner == 1 else -1)\n",
    "\n",
    "            # 学習\n",
    "            agent1.learn(obs, act, rwd, done, next_obs)\n",
    "            \n",
    "        elif env.current_player == -1:\n",
    "            act = agent2.act(obs)\n",
    "            # 状態を更新\n",
    "            env.step(act)\n",
    "            next_obs = env.board\n",
    "            done = env.done()\n",
    "            rwd = 0 if env.winner == 0 else (1 if env.winner == -1 else -1)\n",
    "\n",
    "            # 学習\n",
    "            agent2.learn(obs, act, rwd, done, next_obs)\n",
    "\n",
    "        if done:\n",
    "            # 初期化\n",
    "            env.game_reset()\n",
    "            agent2.episode += 1\n",
    "            print(f\"episode:{agent2.episode}/len_Q:{agent2.len_Q}\", end='\\r')\n",
    "        \n",
    "        if agent2.episode >= 1000000:\n",
    "            running = False\n",
    "    \n",
    "    # 重みパラメータの保存\n",
    "    agent2.save_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede0ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:71233/len_Q:7568\r"
     ]
    }
   ],
   "source": [
    "trainQL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cf0e9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner: O\n",
      "Winner: O\n",
      "Winner: O\n",
      "Winner: O\n",
      "Winner: O\n",
      "Winner: X\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "size = 3\n",
    "agent1 = HumanAgent(size, 1)\n",
    "agent2 = QLAgent(size,-1,epsilon=0.0)\n",
    "agent2.load_weights('agt_data/tictactoe_QL')\n",
    "progress(agent1,agent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadf7fa2",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76ae295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from collections import deque\n",
    "import random\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class Memory:\n",
    "    \"\"\"経験再生のメモリクラス\"\"\"\n",
    "    def __init__(self, memory_size=100, batch_size=30):\n",
    "        self.memory_size = memory_size\n",
    "        self.buffer = deque(maxlen=memory_size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        # 右側に経験を追加\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # バッチサイズ分の経験をサンプリングする\n",
    "        idx = random.sample(range(len(self.buffer)), batch_size)\n",
    "        return [self.buffer[i] for i in idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, size, \n",
    "                 gamma = 0.9, # 割引率\n",
    "                 epsilon = 0.1, # 乱雑度\n",
    "                 memory_size = 1, # 経験の保存数\n",
    "                 batch_size = 1, # 学習で使用する経験の数\n",
    "                 target_interval = 1 # ターゲットを更新する間隔\n",
    "                ):\n",
    "        \n",
    "        # パラメータ\n",
    "        self.input_size = (size, size)\n",
    "        self.n_act = size**2\n",
    "        self.gamma = gamma \n",
    "        self.epsilon = epsilon  \n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.target_interval = target_interval\n",
    "        self.model = self._build_Qnet()\n",
    "        self.time = 0\n",
    "        \n",
    "        # 学習過程の記録関連\n",
    "        self.hist_rwds = []\n",
    "        self.hist_wnrs = []\n",
    "        \n",
    "        # ターゲットモデルの生成\n",
    "        self.model_target = self._build_Qnet()\n",
    "        # メモリのインスタンスを作成\n",
    "        self.memory = Memory(memory_size=self.memory_size, batch_size=batch_size)\n",
    "\n",
    "    def _build_Qnet(self):\n",
    "        # Qネットワークの構築\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=self.input_size))\n",
    "        model.add(Dense(32))\n",
    "        model.add(Dense(32))\n",
    "        model.add(Dense(self.n_act, activation='linear'))\n",
    "        \n",
    "        # 勾配法のパラメータの定義\n",
    "        model.compile(loss='mse', optimizer='Adam')\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def act(self, obs):\n",
    "        # 確率でε-greedy法ではない\n",
    "        if random.random() <= self.epsilon:\n",
    "            act = random.randrange(self.n_act)\n",
    "        else:\n",
    "            # Q値を予測する\n",
    "            Q = self.get_Q(obs)\n",
    "            act = Q.index(max(Q))  # 最大となるQ値を出力\n",
    "        return act\n",
    "    \n",
    "    def get_Q(self, obs, type='main'):\n",
    "        # obsを入力し出力を得る\n",
    "        obs_reshaped = np.array([obs])\n",
    "        if type == 'main':\n",
    "            # Qネットワークに観測obsを入力し出力を得る\n",
    "            Q = self.model.predict(obs_reshaped, verbose=0)[0, :]\n",
    "        elif type == 'target':\n",
    "            # ターゲットネットに観測obsを入力し出力を得る\n",
    "            Q = self.model_target.predict(obs_reshaped, verbose=0)[0, :]\n",
    "\n",
    "        return Q.tolist()\n",
    "             \n",
    "\n",
    "    def learn(self, obs, act, rwd, done, next_obs):\n",
    "        if rwd is None:\n",
    "            return\n",
    "        \n",
    "        self.memory.add((obs, act, rwd, done, next_obs))\n",
    "        \n",
    "        # 学習\n",
    "        self._fit()\n",
    "\n",
    "        # target_intervalの周期でQネットワークの重みをターゲットネットにコピー\n",
    "        if self.time % self.target_interval == 0 and self.time > 0:\n",
    "            self.model_target.set_weights(self.model.get_weights())\n",
    "        print(self.time, end='\\r')\n",
    "\n",
    "        self.time += 1\n",
    "        \n",
    "    def _fit(self):\n",
    "        # 記憶された「経験」のデータの量がバッチサイズに満たない場合は戻る\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # 学習に使うデータを出力\n",
    "        outs = self.memory.sample(self.batch_size)\n",
    "\n",
    "        # 観測とターゲットのバッチを入れる配列を準備\n",
    "        obs_shape = self.input_size\n",
    "        obss = np.zeros((self.batch_size,) + obs_shape)\n",
    "        targets = np.zeros((self.batch_size, self.n_act))\n",
    "        \n",
    "        for i, (obs, act, rwd, done, next_obs) in enumerate(outs):\n",
    "            # obs に対するQネットワークの出力 yを得る\n",
    "            y = self.get_Q(obs)\n",
    "\n",
    "            # target にyの内容をコピーする\n",
    "            target = y[:]\n",
    "\n",
    "            if not done:\n",
    "                # 最終状態でなかったら next_obsに対する next_yを得る\n",
    "                next_y = self.get_Q(next_obs)\n",
    "\n",
    "                # Q[obs][act]のtarget_act を作成\n",
    "                target_act = rwd + self.gamma * max(next_y)\n",
    "            else:\n",
    "                # 最終状態の場合は報酬だけでtarget_actを作成\n",
    "                target_act = rwd\n",
    "\n",
    "            # targetのactの要素だけtarget_actにする\n",
    "            target[act] = target_act\n",
    "\n",
    "            # obsとtargetをバッチの配列に入れる\n",
    "            obss[i] = obs\n",
    "            targets[i] = target\n",
    "        \n",
    "        # obssと targets のバッチのペアを与えて学習\n",
    "        self.model.fit(obss, targets, verbose=0, epochs=1)\n",
    "\n",
    "    \n",
    "    def save_weights(self, filepath='agt_data/noname'):\n",
    "        self.model.save(filepath + '.keras', overwrite=True)\n",
    "        \n",
    "        # episodeとrwdをCSVファイルに保存\n",
    "        with open(filepath + '.csv', \"w\", newline='') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            writer.writerow(['episode','reward','winner'])\n",
    "            for i, (rwd, wnr) in enumerate(zip(self.hist_rwds, self.hist_wnrs)):\n",
    "                writer.writerow([i+1, rwd, wnr])\n",
    "\n",
    "    def load_weights(self, filepath='agt_data/noname'):\n",
    "        # モデルの重みを読み込む\n",
    "        self.model = tf.keras.models.load_model(filepath + '.keras')\n",
    "        \n",
    "        # episodeとrwdをCSVファイルから読み込む\n",
    "        with open(filepath + '.csv', \"r\") as csv_file:\n",
    "            reader = csv.reader(csv_file)\n",
    "            next(reader)\n",
    "            for row in reader:\n",
    "                self.hist_rwds.append(float(row[1]))\n",
    "                self.hist_wnrs.append(float(row[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c14f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    env = TicTacToe()\n",
    "\n",
    "    agent1 = DQNAgent(env.size,1)\n",
    "    agent2 = RandomalfaAgent(env.size,-1)\n",
    "    \n",
    "    filepath='agt_data/tictaktoe_DQN'\n",
    "    \n",
    "    episode = 1\n",
    "    \n",
    "    if os.path.exists(filepath + '.keras'):\n",
    "        agent1.load_weights(filepath)\n",
    "        episode += len(agent1.hist_rwds)\n",
    "    \n",
    "    env.pygame_init()\n",
    "    running = True\n",
    "    \n",
    "    while running:\n",
    "        obs = env.board\n",
    "        env.pygame_render(obs)\n",
    "        \n",
    "        if env.current_player == 1:\n",
    "            act = agent1.act(obs)\n",
    "        elif env.current_player == -1:\n",
    "            act = agent2.act(obs)\n",
    "\n",
    "        # 状態を更新\n",
    "        env.step(act)\n",
    "        next_obs = env.board\n",
    "        done = env.done()\n",
    "        rwd = round(env.reward,2)\n",
    "        \n",
    "        # 描画\n",
    "        env.pygame_render(obs)\n",
    "        \n",
    "        # 学習\n",
    "        agent1.learn(obs, act, rwd, done, next_obs)\n",
    "        \n",
    "        if done:\n",
    "            # 記録\n",
    "            agent1.hist_rwds.append(rwd)\n",
    "            agent1.hist_wnrs.append(env.winner)\n",
    "\n",
    "            # 勝率\n",
    "            win = (agent1.hist_wnrs.count(1) / episode) * 100\n",
    "\n",
    "            # 結果を表示\n",
    "            print(f\"episode{episode}/Reward: {rwd}/Win %: {win} \")\n",
    "            # 初期化\n",
    "            env.game_reset()\n",
    "            episode += 1\n",
    "        \n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "        \n",
    "        if episode > 50000:\n",
    "            running = False\n",
    "\n",
    "    pygame.quit()\n",
    "    \n",
    "    # 重みパラメータの保存\n",
    "    agent1.save_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56cfd693",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msy-t\\anaconda3\\envs\\DL\\lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TicTacToe' object has no attribute 'reward'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m next_obs \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mboard\n\u001b[0;32m     30\u001b[0m done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mdone()\n\u001b[1;32m---> 31\u001b[0m rwd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward\u001b[49m,\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# 描画\u001b[39;00m\n\u001b[0;32m     34\u001b[0m env\u001b[38;5;241m.\u001b[39mpygame_render(obs)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TicTacToe' object has no attribute 'reward'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a55665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
